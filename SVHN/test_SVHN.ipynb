{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:35.115532Z",
     "start_time": "2019-12-13T22:12:35.101598Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:36.913628Z",
     "start_time": "2019-12-13T22:12:35.672509Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 857
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 313348,
     "status": "error",
     "timestamp": 1564114306767,
     "user": {
      "displayName": "Jiamian Wang",
      "photoUrl": "",
      "userId": "08424312081130452473"
     },
     "user_tz": 420
    },
    "id": "0vCnygEdWNjW",
    "outputId": "192cb6c0-8859-4f25-b561-7bdb14d6d266"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import utils.lsun_loader as lsun_loader\n",
    "import utils.svhn_loader as svhn\n",
    "from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n",
    "from PIL import Image as PILImage\n",
    "from skimage.filters import gaussian as gblur\n",
    "\n",
    "from CIFAR.models.wrn import WideResNet\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as trn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import time\n",
    "import argparse\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:37.672997Z",
     "start_time": "2019-12-13T22:12:37.545153Z"
    }
   },
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:37.954124Z",
     "start_time": "2019-12-13T22:12:37.938637Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install --user lmdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:38.545924Z",
     "start_time": "2019-12-13T22:12:38.531114Z"
    }
   },
   "outputs": [],
   "source": [
    "cd /home/outlier-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:39.382679Z",
     "start_time": "2019-12-13T22:12:39.367805Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "rVrCoDJwWTeL"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'test_bs': 200,\n",
    "    'num_to_avg': 1,  # 'Average measures across num_to_avg runs.'\n",
    "    'validate':'',  \n",
    "    'use_xent':'',  \n",
    "    'method_name': 'svhn_wrn_OECC_tune',  # 'Method name.' \n",
    "    'layers': 16,\n",
    "    'widen-factor': 4,\n",
    "    'droprate': 0.4,\n",
    "    'load': './SVHN/results',\n",
    "    'save': './SVHN/results',\n",
    "    'ngpu': 1,\n",
    "    'prefetch': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:12:44.949743Z",
     "start_time": "2019-12-13T22:12:44.363088Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8u3jNncSWWcS"
   },
   "outputs": [],
   "source": [
    "root_dir = './SVHN'\n",
    "\n",
    "#torch.manual_seed(1)\n",
    "#np.random.seed(1)\n",
    "\n",
    "test_data = svhn.SVHN(root_dir,\n",
    "                      split='test',\n",
    "                      transform=trn.ToTensor(),\n",
    "                      download=False)\n",
    "num_classes = 10\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=args['test_bs'],\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args['prefetch'],\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T22:14:15.586538Z",
     "start_time": "2019-12-13T22:12:46.202868Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "8NNd5v0uWcYr"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "if 'allconv' in args['method_name']:\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args['layers'],\n",
    "                     num_classes,\n",
    "                     args['widen-factor'],\n",
    "                     dropRate=args['droprate'])\n",
    "\n",
    "start_epoch = 9\n",
    "\n",
    "if 'baseline' in args['method_name']:\n",
    "    subdir = 'baseline'\n",
    "elif 'OECC' in args['method_name']:\n",
    "    subdir = 'OECC_tune'\n",
    "\n",
    "f = open(\n",
    "    os.path.join(os.path.join(args['save'], subdir),\n",
    "                 args['method_name'] + '_test.txt'), 'w+')\n",
    "\n",
    "# Restore model\n",
    "if args['load'] != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(\n",
    "            os.path.join(args['load'], subdir),\n",
    "            args['method_name'] + '_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch:', i)\n",
    "            f.write(f'Model restored! Epoch: {i}')\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "net.eval()\n",
    "\n",
    "if args['ngpu'] > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args['ngpu'])))\n",
    "\n",
    "if args['ngpu'] > 0:\n",
    "    net.cuda()\n",
    "    # torch.cuda.manual_seed(1)\n",
    "\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "# /////////////// Detection Prelims ///////////////\n",
    "\n",
    "ood_num_examples = test_data.data.shape[0] // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + test_data.data.shape[0])\n",
    "\n",
    "\n",
    "def concat(x):\n",
    "    return np.concatenate(x, axis=0)\n",
    "\n",
    "\n",
    "def to_np(x):\n",
    "    return x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_ood_scores(loader, in_dist=False):\n",
    "    _score = []\n",
    "    _right_score = []\n",
    "    _wrong_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            if batch_idx >= ood_num_examples // args[\n",
    "                    'test_bs'] and in_dist is False:\n",
    "                break\n",
    "\n",
    "            data = data.cuda()\n",
    "\n",
    "            output = net(data)\n",
    "            smax = to_np(F.softmax(output, dim=1))\n",
    "\n",
    "            if args['use_xent']:\n",
    "                _score.append(\n",
    "                    to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n",
    "            else:\n",
    "                _score.append(-np.max(smax, axis=1))\n",
    "\n",
    "            if in_dist:\n",
    "                preds = np.argmax(smax, axis=1)\n",
    "                targets = target.numpy().squeeze()\n",
    "                right_indices = preds == targets\n",
    "                wrong_indices = np.invert(right_indices)\n",
    "\n",
    "                if args['use_xent']:\n",
    "                    _right_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[right_indices])\n",
    "                    _wrong_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[wrong_indices])\n",
    "                else:\n",
    "                    _right_score.append(-np.max(smax[right_indices], axis=1))\n",
    "                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n",
    "\n",
    "    if in_dist:\n",
    "        return concat(_score).copy(), concat(_right_score).copy(), concat(\n",
    "            _wrong_score).copy()\n",
    "    else:\n",
    "        return concat(_score)[:ood_num_examples].copy()\n",
    "\n",
    "\n",
    "in_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n",
    "\n",
    "num_right = len(right_score)\n",
    "num_wrong = len(wrong_score)\n",
    "print('Error Rate {:.2f}'.format(100 * num_wrong / (num_wrong + num_right)))\n",
    "f.write('\\nError Rate {:.2f}'.format(100 * num_wrong /\n",
    "                                     (num_wrong + num_right)))\n",
    "\n",
    "# /////////////// End Detection Prelims ///////////////\n",
    "\n",
    "print('\\nUsing SVHN as typical data')\n",
    "f.write('\\nUsing SVHN as typical data')\n",
    "\n",
    "# /////////////// Error Detection ///////////////\n",
    "\n",
    "# print('\\n\\nError Detection')\n",
    "# f.write('\\n\\nError Detection')\n",
    "# show_performance(wrong_score, right_score, f, method_name=args['method_name'])\n",
    "\n",
    "# /////////////// OOD Detection ///////////////\n",
    "auroc_list, aupr_list, fpr_list = [], [], []\n",
    "\n",
    "\n",
    "def get_and_print_results(ood_loader, num_to_avg=args['num_to_avg']):\n",
    "\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    for _ in range(num_to_avg):\n",
    "        out_score = get_ood_scores(ood_loader)\n",
    "        measures = get_measures(out_score, in_score)\n",
    "        aurocs.append(measures[0])\n",
    "        auprs.append(measures[1])\n",
    "        fprs.append(measures[2])\n",
    "\n",
    "    auroc = np.mean(aurocs)\n",
    "    aupr = np.mean(auprs)\n",
    "    fpr = np.mean(fprs)\n",
    "    auroc_list.append(auroc)\n",
    "    aupr_list.append(aupr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "    if num_to_avg >= 5:\n",
    "        print_measures_with_std(aurocs, auprs, fprs, f, args['method_name'])\n",
    "    else:\n",
    "        print_measures(auroc, aupr, fpr, f, args['method_name'])\n",
    "\n",
    "\n",
    "# /////////////// Gaussian Noise ///////////////\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(\n",
    "    np.float32(\n",
    "        np.clip(\n",
    "            np.random.normal(size=(ood_num_examples * args['num_to_avg'], 3,\n",
    "                                   32, 32),\n",
    "                             loc=0.5,\n",
    "                             scale=0.5).astype(np.float32), 0, 1)))\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True)\n",
    "\n",
    "print('\\n\\nGaussian Noise Detection (sigma = 0.5)')\n",
    "f.write('\\n\\nGaussian Noise Detection (sigma = 0.5)')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Bernoulli  Noise ///////////////\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(\n",
    "    np.random.binomial(n=1,\n",
    "                       p=0.5,\n",
    "                       size=(ood_num_examples * args['num_to_avg'], 3, 32,\n",
    "                             32)).astype(np.float32))\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True)\n",
    "\n",
    "print('\\n\\nBernoulli Noise Detection')\n",
    "f.write('\\n\\nBernoulli Noise Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Blob ///////////////\n",
    "\n",
    "ood_data = np.float32(\n",
    "    np.random.binomial(n=1,\n",
    "                       p=0.7,\n",
    "                       size=(ood_num_examples * args['num_to_avg'], 32, 32,\n",
    "                             3)))\n",
    "for i in range(ood_num_examples * args['num_to_avg']):\n",
    "    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n",
    "    ood_data[i][ood_data[i] < 0.75] = 0.0\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2)))\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nBlob Detection')\n",
    "f.write('\\n\\nBlob Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Icons-50 ///////////////\n",
    "\n",
    "ood_data = dset.ImageFolder(root=\"Icons-50\",\n",
    "                            transform=trn.Compose(\n",
    "                                [trn.Resize((32, 32)),\n",
    "                                 trn.ToTensor()]))\n",
    "\n",
    "filtered_imgs = []\n",
    "for img in ood_data.imgs:\n",
    "    if 'numbers' not in img[0]:  # img[0] is image name\n",
    "        filtered_imgs.append(img)\n",
    "ood_data.imgs = filtered_imgs\n",
    "\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args[\"test_bs\"],\n",
    "                                         shuffle=True)\n",
    "\n",
    "print('\\n\\nIcons-50 Detection')\n",
    "f.write('\\n\\nIcons-50 Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Textures ///////////////\n",
    "\n",
    "ood_data = dset.ImageFolder(\n",
    "    root=\"dtd/images\",\n",
    "    transform=trn.Compose([trn.Resize(32),\n",
    "                           trn.CenterCrop(32),\n",
    "                           trn.ToTensor()]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nTexture Detection')\n",
    "f.write('\\n\\nTexture Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Places365 ///////////////\n",
    "\n",
    "ood_data = dset.ImageFolder(\n",
    "    root=\"Places365\",\n",
    "    transform=trn.Compose([trn.Resize(32),\n",
    "                           trn.CenterCrop(32),\n",
    "                           trn.ToTensor()]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nPlaces365 Detection')\n",
    "f.write('\\n\\nPlaces365 Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// LSUN ///////////////\n",
    "\n",
    "ood_data = lsun_loader.LSUN(\n",
    "    \"lsun_dataset\",\n",
    "    classes='test',\n",
    "    transform=trn.Compose([trn.Resize(32),\n",
    "                           trn.CenterCrop(32),\n",
    "                           trn.ToTensor()]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nLSUN Detection')\n",
    "f.write('\\n\\nLSUN Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// CIFAR Data ///////////////\n",
    "\n",
    "ood_data = dset.CIFAR10('CIFAR',\n",
    "                        train=False,\n",
    "                        transform=trn.ToTensor(),\n",
    "                        download=False)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nCIFAR-10 Detection')\n",
    "f.write('\\n\\nCIFAR-10 Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Mean Results ///////////////\n",
    "\n",
    "print('\\n\\nMean Test Results')\n",
    "f.write('\\n\\nMean Test Results')\n",
    "print_measures(np.mean(auroc_list),\n",
    "               np.mean(aupr_list),\n",
    "               np.mean(fpr_list),\n",
    "               f,\n",
    "               method_name=args['method_name'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "test_SVHN.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
