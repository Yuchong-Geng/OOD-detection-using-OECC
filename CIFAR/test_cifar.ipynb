{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T21:08:28.332669Z",
     "start_time": "2019-12-13T21:08:27.077339Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bl53W97H-9c0"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys, argparse, time\n",
    "sys.path.append('..')\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from skimage.filters import gaussian as gblur\n",
    "from PIL import Image as PILImage\n",
    "import seaborn as sns\n",
    "\n",
    "from CIFAR.models.wrn import WideResNet \n",
    "from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n",
    "import utils.svhn_loader as svhn\n",
    "import utils.lsun_loader as lsun_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T21:08:29.205962Z",
     "start_time": "2019-12-13T21:08:29.186328Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/outlier-detection\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T21:08:31.440216Z",
     "start_time": "2019-12-13T21:08:31.422863Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "677EXjGf_nl7"
   },
   "outputs": [],
   "source": [
    " args = {\n",
    "        'test_bs': 200,\n",
    "        'num_to_avg': 1, # 'Average measures across num_to_avg runs.' \n",
    "        'validate': '', \n",
    "        'use_xent': '', \n",
    "        'method_name': 'cifar10_wrn_OECC_tune', # 'Method name.'\n",
    "        'layers': 40,\n",
    "        'widen-factor': 2,\n",
    "        'droprate': 0.3,\n",
    "        'load': './CIFAR/results',\n",
    "        'save': './CIFAR/results',\n",
    "        'ngpu': 2,\n",
    "        'prefetch': 4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T21:15:40.185861Z",
     "start_time": "2019-12-13T21:15:39.453190Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1563589202392,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "MIAygnEDd12K",
    "outputId": "54ba538b-0c42-4290-f2a1-5dca8692c902"
   },
   "outputs": [],
   "source": [
    "root_dir = 'CIFAR'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# mean and standard deviation of channels of CIFAR-10 images\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "if 'cifar10_' in args['method_name']:\n",
    "    test_data = dset.CIFAR10(root_dir,\n",
    "                             train=False,\n",
    "                             download=True,\n",
    "                             transform=test_transform)\n",
    "    num_classes = 10\n",
    "else:\n",
    "    test_data = dset.CIFAR100(root_dir,\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=test_transform)\n",
    "    num_classes = 100\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=args['test_bs'],\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args['prefetch'],\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-13T21:17:09.154709Z",
     "start_time": "2019-12-13T21:16:39.985658Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 773601,
     "status": "error",
     "timestamp": 1563589976125,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "u6nimDNHMthq",
    "outputId": "75a7924c-df49-4359-a8c7-43bbd9c9865c"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "if 'allconv' in args['method_name']:\n",
    "    net = AllConvNet(num_classes)\n",
    "else:\n",
    "    net = WideResNet(args['layers'],\n",
    "                     num_classes,\n",
    "                     args['widen-factor'],\n",
    "                     dropRate=args['droprate'])\n",
    "\n",
    "if args['ngpu'] > 1:\n",
    "    net = torch.nn.DataParallel(net, device_ids=list(range(args['ngpu'])))\n",
    "    \n",
    "start_epoch = 0\n",
    "\n",
    "if 'baseline' in args['method_name']:\n",
    "    subdir = 'baseline'\n",
    "elif 'OECC' in args['method_name']:\n",
    "    subdir = 'OECC_tune'\n",
    "\n",
    "f = open(\n",
    "    os.path.join(os.path.join(args['save'], subdir),\n",
    "                 args['method_name'] + '_test.txt'), 'w+')\n",
    "\n",
    "# Restore model\n",
    "if args['load'] != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(\n",
    "            os.path.join(args['load'], subdir),\n",
    "            args['method_name'] + '_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch: ', i)\n",
    "            f.write('Model restored! Epoch: {}'.format(i))\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "net.eval()\n",
    "\n",
    "\n",
    "if args['ngpu'] > 0:\n",
    "    torch.cuda.set_device(0)    \n",
    "    device = torch.device('cuda:0')    \n",
    "    net = net.cuda()\n",
    "    # torch.cuda.manual_seed(1)\n",
    "\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "# /////////////// Detection Prelims ///////////////\n",
    "\n",
    "ood_num_examples = len(test_data) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n",
    "\n",
    "concat = lambda x: np.concatenate(x, axis=0)\n",
    "to_np = lambda x: x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_ood_scores(loader, in_dist=False):\n",
    "    _score = []\n",
    "    out_conf_score = []\n",
    "    in_conf_score = []\n",
    "    _right_score = []\n",
    "    _wrong_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            if batch_idx >= ood_num_examples // args[\n",
    "                    'test_bs'] and in_dist is False:\n",
    "                break\n",
    "\n",
    "#             data = data.cuda(device)\n",
    "            data = data.cuda()\n",
    "\n",
    "            output = net(data)\n",
    "            smax = to_np(F.softmax(output, dim=1))\n",
    "\n",
    "            if args['use_xent']:\n",
    "                _score.append(\n",
    "                    to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n",
    "            else:\n",
    "                _score.append(-np.max(smax, axis=1))\n",
    "                out_conf_score.append(np.max(smax, axis=1))\n",
    "\n",
    "            if in_dist:\n",
    "                in_conf_score.append(np.max(smax, axis=1))\n",
    "                preds = np.argmax(smax, axis=1)\n",
    "                targets = target.numpy().squeeze()\n",
    "                right_indices = preds == targets\n",
    "                wrong_indices = np.invert(right_indices)\n",
    "\n",
    "                if args['use_xent']:\n",
    "                    _right_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[right_indices])\n",
    "                    _wrong_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[wrong_indices])\n",
    "                else:\n",
    "                    _right_score.append(-np.max(smax[right_indices], axis=1))\n",
    "                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n",
    "\n",
    "    if in_dist:\n",
    "        return concat(in_conf_score).copy(), concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n",
    "    else:\n",
    "        return concat(out_conf_score).copy(), concat(_score)[:ood_num_examples].copy()\n",
    "\n",
    "\n",
    "in_conf_score, in_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n",
    "\n",
    "num_right = len(right_score)\n",
    "num_wrong = len(wrong_score)\n",
    "print('Error Rate {:.2f}'.format(100 * num_wrong / (num_wrong + num_right)))\n",
    "f.write('\\nError Rate {:.2f}'.format(100 * num_wrong /\n",
    "                                     (num_wrong + num_right)))\n",
    "\n",
    "# /////////////// End Detection Prelims ///////////////\n",
    "\n",
    "print('\\nUsing CIFAR-10 as typical data') if num_classes == 10 else print(\n",
    "    '\\nUsing CIFAR-100 as typical data')\n",
    "f.write('\\nUsing CIFAR-10 as typical data') if num_classes == 10 else f.write(\n",
    "    '\\nUsing CIFAR-100 as typical data')\n",
    "\n",
    "# /////////////// Error Detection ///////////////\n",
    "\n",
    "# print('\\n\\nError Detection')\n",
    "# f.write('\\n\\nError Detection')\n",
    "# show_performance(wrong_score, right_score, f, method_name=args['method_name'])\n",
    "\n",
    "# /////////////// OOD Detection ///////////////\n",
    "auroc_list, aupr_list, fpr_list = [], [], []\n",
    "\n",
    "\n",
    "def get_and_print_results(ood_loader, num_to_avg=args['num_to_avg']):\n",
    "\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    for _ in range(num_to_avg):\n",
    "        out_conf_score, out_score = get_ood_scores(ood_loader)\n",
    "        measures = get_measures(out_score, in_score)\n",
    "        aurocs.append(measures[0])\n",
    "        auprs.append(measures[1])\n",
    "        fprs.append(measures[2])\n",
    "\n",
    "    auroc = np.mean(aurocs)\n",
    "    aupr = np.mean(auprs)\n",
    "    fpr = np.mean(fprs)\n",
    "    auroc_list.append(auroc)\n",
    "    aupr_list.append(aupr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "    if num_to_avg >= 5:\n",
    "        print_measures_with_std(aurocs, auprs, fprs, f, args['method_name'])\n",
    "    else:\n",
    "        print_measures(auroc, aupr, fpr, f, args['method_name'])\n",
    "    return out_conf_score    \n",
    "\n",
    "\n",
    "# /////////////// Gaussian Noise ///////////////\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(\n",
    "    np.float32(\n",
    "        np.clip(\n",
    "            np.random.normal(size=(ood_num_examples * args['num_to_avg'], 3,\n",
    "                                   32, 32),\n",
    "                             scale=0.5), -1, 1)))\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nGaussian Noise (sigma = 0.5) Detection')\n",
    "f.write('\\n\\nGaussian Noise (sigma = 0.5) Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "#/////////////// Rademacher Noise ///////////////\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(\n",
    "    np.random.binomial(\n",
    "        n=1, p=0.5, size=(ood_num_examples * args['num_to_avg'], 3, 32,\n",
    "                          32)).astype(np.float32)) * 2 - 1\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True)\n",
    "\n",
    "print('\\n\\nRademacher Noise Detection')\n",
    "f.write('\\n\\nRademacher Noise Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Blob ///////////////\n",
    "\n",
    "ood_data = np.float32(\n",
    "    np.random.binomial(n=1,\n",
    "                       p=0.7,\n",
    "                       size=(ood_num_examples * args['num_to_avg'], 32, 32,\n",
    "                             3)))\n",
    "for i in range(ood_num_examples * args['num_to_avg']):\n",
    "    ood_data[i] = gblur(ood_data[i], sigma=1.5, multichannel=False)\n",
    "    ood_data[i][ood_data[i] < 0.75] = 0.0\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(ood_data.transpose((0, 3, 1, 2))) * 2 - 1\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nBlob Detection')\n",
    "f.write('\\n\\nBlob Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Textures ///////////////\n",
    "\n",
    "ood_data = dset.ImageFolder(root=\"./dtd/images\",\n",
    "                            transform=trn.Compose([\n",
    "                                trn.Resize(32),\n",
    "                                trn.CenterCrop(32),\n",
    "                                trn.ToTensor(),\n",
    "                                trn.Normalize(mean, std)\n",
    "                            ]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nTexture Detection')\n",
    "f.write('\\n\\nTexture Detection')\n",
    "texture_out_score = get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// SVHN ///////////////\n",
    "\n",
    "ood_data = svhn.SVHN(root='SVHN',\n",
    "                     split=\"test\",\n",
    "                     transform=trn.Compose([\n",
    "                         trn.Resize(32),\n",
    "                         trn.ToTensor(),\n",
    "                         trn.Normalize(mean, std)\n",
    "                     ]),\n",
    "                     download=True)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nSVHN Detection')\n",
    "f.write('\\n\\nSVHN Detection')\n",
    "svhn_out_score = get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Places365 ///////////////\n",
    "ood_data = dset.ImageFolder(root=\"./Places365/\",\n",
    "                            transform=trn.Compose([\n",
    "                                trn.Resize(32),\n",
    "                                trn.CenterCrop(32),\n",
    "                                trn.ToTensor(),\n",
    "                                trn.Normalize(mean, std)\n",
    "                            ]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nPlaces365 Detection')\n",
    "f.write('\\n\\nPlaces365 Detection')\n",
    "places_out_score = get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// LSUN ///////////////\n",
    "\n",
    "ood_data = lsun_loader.LSUN(\"./lsun_dataset\",\n",
    "                            classes='test',\n",
    "                            transform=trn.Compose([\n",
    "                                trn.Resize(32),\n",
    "                                trn.CenterCrop(32),\n",
    "                                trn.ToTensor(),\n",
    "                                trn.Normalize(mean, std)\n",
    "                            ]))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print('\\n\\nLSUN Detection')\n",
    "f.write('\\n\\nLSUN Detection')\n",
    "lsun_out_score = get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// CIFAR Data ///////////////\n",
    "\n",
    "train_transform = trn.Compose([\n",
    "    trn.RandomHorizontalFlip(),\n",
    "    trn.RandomCrop(32, padding=4),\n",
    "    trn.ToTensor(),\n",
    "    trn.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "if 'cifar10_' in args['method_name']:\n",
    "    ood_data = dset.CIFAR100(root_dir,\n",
    "                             train=False,\n",
    "                             download=True,\n",
    "                             transform=train_transform)\n",
    "else:\n",
    "    ood_data = dset.CIFAR10(root_dir,\n",
    "                            train=False,\n",
    "                            download=True,\n",
    "                            transform=train_transform)\n",
    "\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data,\n",
    "                                         batch_size=args['test_bs'],\n",
    "                                         shuffle=True,\n",
    "                                         num_workers=args['prefetch'],\n",
    "                                         pin_memory=True)\n",
    "\n",
    "print(\n",
    "    '\\n\\nCIFAR-100 Detection') if 'cifar10_' in args['method_name'] else print(\n",
    "        '\\n\\nCIFAR-10 Detection')\n",
    "f.write('\\n\\nCIFAR-100 Detection'\n",
    "        ) if 'cifar10_' in args['method_name'] else f.write(\n",
    "            '\\n\\nCIFAR-10 Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Mean Results ///////////////\n",
    "\n",
    "print('\\n\\nMean Test Results')\n",
    "f.write('\\n\\nMean Test Results')\n",
    "print_measures(np.mean(auroc_list),\n",
    "               np.mean(aupr_list),\n",
    "               np.mean(fpr_list),\n",
    "               f,\n",
    "               method_name=args['method_name'])\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-08T04:49:24.787133Z",
     "start_time": "2019-11-08T04:49:24.355472Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-notebook')\n",
    "plt.hist(in_conf_score[:1000], bins=10)\n",
    "plt.hist(places_out_score[:1000], bins=10)\n",
    "plt.legend(labels=['cifar10', 'places365'],loc='upper center', prop={'size': 20})\n",
    "plt.ylabel('Number of examples', fontdict={'fontsize': 20})\n",
    "plt.xlabel('Softmax probablities', fontdict={'fontsize': 20})\n",
    "plt.xticks(size=20)\n",
    "plt.yticks(size=20)\n",
    "# plt.figure(background_color='white')\n",
    "plt.savefig('ours_cifar10_places365_distribution.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "test_cifar.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
