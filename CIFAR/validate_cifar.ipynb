{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-30T23:56:27.687560Z",
     "start_time": "2019-10-30T23:56:26.323407Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "bl53W97H-9c0"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import os, sys, argparse, time\n",
    "sys.path.append('..')\n",
    "\n",
    "import pickle\n",
    "import torch\n",
    "import seaborn as sns\n",
    "import torch.nn as nnll\n",
    "import numpy as np\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from skimage.filters import gaussian as gblur\n",
    "from PIL import Image as PILImage\n",
    "import seaborn as sns\n",
    "\n",
    "from CIFAR.models.wrn import WideResNet \n",
    "from utils.display_results import show_performance, get_measures, print_measures, print_measures_with_std\n",
    "import utils.svhn_loader as svhn\n",
    "import utils.lsun_loader as lsun_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T18:52:14.404799Z",
     "start_time": "2019-10-31T18:52:14.386938Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "677EXjGf_nl7"
   },
   "outputs": [],
   "source": [
    " args = {\n",
    "        'test_bs': 200,\n",
    "        'num_to_avg': 1, # 'Average measures across num_to_avg runs.' \n",
    "        'validate': '', \n",
    "        'use_xent': '',  \n",
    "        'method_name': 'cifar10_wrn_OECC_tune', # 'Method name.'\n",
    "        'layers': 40,\n",
    "        'widen-factor': 2,\n",
    "        'droprate': 0.3,\n",
    "        'load': './results',\n",
    "        'save': './results',\n",
    "        'ngpu': 2,\n",
    "        'prefetch': 4\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T18:52:15.480438Z",
     "start_time": "2019-10-31T18:52:14.756030Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1945,
     "status": "ok",
     "timestamp": 1563589202392,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "MIAygnEDd12K",
    "outputId": "54ba538b-0c42-4290-f2a1-5dca8692c902"
   },
   "outputs": [],
   "source": [
    "root_dir = './'\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# mean and standard deviation of channels of CIFAR-10 images\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "if 'cifar10_' in args['method_name']:\n",
    "    test_data = dset.CIFAR10(root_dir,\n",
    "                             train=False,\n",
    "                             download=True,\n",
    "                             transform=test_transform)\n",
    "    num_classes = 10\n",
    "else:\n",
    "    test_data = dset.CIFAR100(root_dir,\n",
    "                              train=False,\n",
    "                              download=True,\n",
    "                              transform=test_transform)\n",
    "    num_classes = 100\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data,\n",
    "                                          batch_size=args['test_bs'],\n",
    "                                          shuffle=False,\n",
    "                                          num_workers=args['prefetch'],\n",
    "                                          pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-10-31T18:52:30.405445Z",
     "start_time": "2019-10-31T18:52:15.612091Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 773601,
     "status": "error",
     "timestamp": 1563589976125,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "u6nimDNHMthq",
    "outputId": "75a7924c-df49-4359-a8c7-43bbd9c9865c"
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "net = WideResNet(args['layers'], num_classes, args['widen_factor'], dropRate=args['droprate'])\n",
    "    \n",
    "device = torch.device('cuda:0')    \n",
    "net = torch.nn.DataParallel(net, device_ids=list(range(args['ngpu']))).cuda()\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "start_epoch = 9\n",
    "\n",
    "if 'baseline' in args['method_name']:\n",
    "    subdir = 'baseline'\n",
    "elif 'OECC' in args['method_name']:\n",
    "    subdir = 'OECC_tune'\n",
    "\n",
    "f = open(\n",
    "    os.path.join(os.path.join(args['save'], subdir),\n",
    "                 args['method_name'] + '_test.txt'), 'w+')\n",
    "\n",
    "# Restore model\n",
    "if args['load'] != '':\n",
    "    for i in range(1000 - 1, -1, -1):\n",
    "        model_name = os.path.join(\n",
    "            os.path.join(args['load'], subdir),\n",
    "            args['method_name'] + '_epoch_' + str(i) + '.pt')\n",
    "        if os.path.isfile(model_name):\n",
    "            net.load_state_dict(torch.load(model_name))\n",
    "            print('Model restored! Epoch: ', i)\n",
    "            f.write('Model restored! Epoch: {}'.format(i))\n",
    "            start_epoch = i + 1\n",
    "            break\n",
    "    if start_epoch == 0:\n",
    "        assert False, \"could not resume\"\n",
    "\n",
    "net.eval()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# /////////////// Detection Prelims ///////////////\n",
    "\n",
    "ood_num_examples = len(test_data) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_data))\n",
    "\n",
    "concat = lambda x: np.concatenate(x, axis=0)\n",
    "to_np = lambda x: x.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_ood_scores(loader, in_dist=False):\n",
    "    _score = []\n",
    "    out_conf_score = []\n",
    "    in_conf_score = []\n",
    "    _right_score = []\n",
    "    _wrong_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, target) in enumerate(loader):\n",
    "            if batch_idx >= ood_num_examples // args[\n",
    "                    'test_bs'] and in_dist is False:\n",
    "                break\n",
    "\n",
    "            data = data.cuda()\n",
    "\n",
    "            output = net(data)\n",
    "            smax = to_np(F.softmax(output, dim=1))\n",
    "\n",
    "            if args['use_xent']:\n",
    "                _score.append(\n",
    "                    to_np((output.mean(1) - torch.logsumexp(output, dim=1))))\n",
    "            else:\n",
    "                _score.append(-np.max(smax, axis=1))\n",
    "                out_conf_score.append(np.max(smax, axis=1))\n",
    "\n",
    "            if in_dist:\n",
    "                in_conf_score.append(np.max(smax, axis=1))\n",
    "                preds = np.argmax(smax, axis=1)\n",
    "                targets = target.numpy().squeeze()\n",
    "                right_indices = preds == targets\n",
    "                wrong_indices = np.invert(right_indices)\n",
    "\n",
    "                if args['use_xent']:\n",
    "                    _right_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[right_indices])\n",
    "                    _wrong_score.append(\n",
    "                        to_np((output.mean(1) -\n",
    "                               torch.logsumexp(output, dim=1)))[wrong_indices])\n",
    "                else:\n",
    "                    _right_score.append(-np.max(smax[right_indices], axis=1))\n",
    "                    _wrong_score.append(-np.max(smax[wrong_indices], axis=1))\n",
    "\n",
    "    if in_dist:\n",
    "        return concat(in_conf_score).copy(), concat(_score).copy(), concat(_right_score).copy(), concat(_wrong_score).copy()\n",
    "    else:\n",
    "        return concat(out_conf_score).copy(), concat(_score)[:ood_num_examples].copy()\n",
    "\n",
    "\n",
    "in_conf_score, in_score, right_score, wrong_score = get_ood_scores(test_loader, in_dist=True)\n",
    "\n",
    "num_right = len(right_score)\n",
    "num_wrong = len(wrong_score)\n",
    "print('Error Rate {:.2f}'.format(100 * num_wrong / (num_wrong + num_right)))\n",
    "# f.write('\\nError Rate {:.2f}'.format(100 * num_wrong /\n",
    "#                                      (num_wrong + num_right)))\n",
    "\n",
    "# /////////////// End Detection Prelims ///////////////\n",
    "\n",
    "print('\\nUsing CIFAR-10 as typical data') if num_classes == 10 else print(\n",
    "    '\\nUsing CIFAR-100 as typical data')\n",
    "f.write('\\nUsing CIFAR-10 as typical data') if num_classes == 10 else f.write(\n",
    "    '\\nUsing CIFAR-100 as typical data')\n",
    "\n",
    "auroc_list, aupr_list, fpr_list = [], [], []\n",
    "\n",
    "\n",
    "def get_and_print_results(ood_loader, num_to_avg=args['num_to_avg']):\n",
    "\n",
    "    aurocs, auprs, fprs = [], [], []\n",
    "    for _ in range(num_to_avg):\n",
    "        out_conf_score, out_score = get_ood_scores(ood_loader)\n",
    "        measures = get_measures(out_score, in_score)\n",
    "        aurocs.append(measures[0])\n",
    "        auprs.append(measures[1])\n",
    "        fprs.append(measures[2])\n",
    "\n",
    "    auroc = np.mean(aurocs)\n",
    "    aupr = np.mean(auprs)\n",
    "    fpr = np.mean(fprs)\n",
    "    auroc_list.append(auroc)\n",
    "    aupr_list.append(aupr)\n",
    "    fpr_list.append(fpr)\n",
    "\n",
    "    if num_to_avg >= 5:\n",
    "        print_measures_with_std(aurocs, auprs, fprs, f, args['method_name'])\n",
    "    else:\n",
    "        print_measures(auroc, aupr, fpr, f, args['method_name'])\n",
    "    return out_conf_score\n",
    "\n",
    "# /////////////// Uniform Noise ///////////////\n",
    "\n",
    "dummy_targets = torch.ones(ood_num_examples * args['num_to_avg'])\n",
    "ood_data = torch.from_numpy(\n",
    "    np.random.uniform(size=(ood_num_examples * args['num_to_avg'], 3, 32, 32),\n",
    "                      low=-1.0, high=1.0).astype(np.float32))\n",
    "ood_data = torch.utils.data.TensorDataset(ood_data, dummy_targets)\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args['test_bs'], shuffle=True)\n",
    "\n",
    "print('\\n\\nUniform[-1,1] Noise Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "\n",
    "# /////////////// Arithmetic Mean of Images ///////////////\n",
    "\n",
    "if 'cifar100' in args['method_name']:\n",
    "    ood_data = dset.CIFAR100('.', download=True, train=False, transform=test_transform)\n",
    "else:\n",
    "    ood_data = dset.CIFAR10('.', download=True,train=False, transform=test_transform)\n",
    "\n",
    "\n",
    "class AvgOfPair(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle_indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(self.shuffle_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        random_idx = np.random.choice(len(self.dataset))\n",
    "        while random_idx == i:\n",
    "            random_idx = np.random.choice(len(self.dataset))\n",
    "\n",
    "        return self.dataset[i][0] / 2. + self.dataset[random_idx][0] / 2., 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "ood_loader = torch.utils.data.DataLoader(AvgOfPair(ood_data),\n",
    "                                         batch_size=args['test_bs'], shuffle=True,\n",
    "                                         num_workers=args['prefetch'], pin_memory=True)\n",
    "\n",
    "print('\\n\\nArithmetic Mean of Random Image Pair Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "\n",
    "# /////////////// Geometric Mean of Images ///////////////\n",
    "\n",
    "if 'cifar10_' in args['method_name']:\n",
    "    ood_data = dset.CIFAR100('.', download=True, train=False, transform=trn.ToTensor())\n",
    "else:\n",
    "    ood_data = dset.CIFAR10('.', download=True,train=False, transform=trn.ToTensor())\n",
    "\n",
    "\n",
    "class GeomMeanOfPair(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.dataset = dataset\n",
    "        self.shuffle_indices = np.arange(len(dataset))\n",
    "        np.random.shuffle(self.shuffle_indices)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        random_idx = np.random.choice(len(self.dataset))\n",
    "        while random_idx == i:\n",
    "            random_idx = np.random.choice(len(self.dataset))\n",
    "\n",
    "        return trn.Normalize(mean, std)(torch.sqrt(self.dataset[i][0] * self.dataset[random_idx][0])), 0\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "ood_loader = torch.utils.data.DataLoader(\n",
    "    GeomMeanOfPair(ood_data), batch_size=args['test_bs'], shuffle=True,\n",
    "    num_workers=args['prefetch'], pin_memory=True)\n",
    "\n",
    "print('\\n\\nGeometric Mean of Random Image Pair Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Jigsaw Images ///////////////\n",
    "\n",
    "ood_loader = torch.utils.data.DataLoader(ood_data, batch_size=args['test_bs'], shuffle=True,\n",
    "                                         num_workers=args['prefetch'], pin_memory=True)\n",
    "\n",
    "jigsaw = lambda x: torch.cat((\n",
    "    torch.cat((torch.cat((x[:, 8:16, :16], x[:, :8, :16]), 1),\n",
    "               x[:, 16:, :16]), 2),\n",
    "    torch.cat((x[:, 16:, 16:],\n",
    "               torch.cat((x[:, :16, 24:], x[:, :16, 16:24]), 2)), 2),\n",
    "), 1)\n",
    "\n",
    "ood_loader.dataset.transform = trn.Compose([trn.ToTensor(), jigsaw, trn.Normalize(mean, std)])\n",
    "\n",
    "print('\\n\\nJigsawed Images Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Speckled Images ///////////////\n",
    "\n",
    "speckle = lambda x: torch.clamp(x + x * torch.randn_like(x), 0, 1)\n",
    "ood_loader.dataset.transform = trn.Compose([trn.ToTensor(), speckle, trn.Normalize(mean, std)])\n",
    "\n",
    "print('\\n\\nSpeckle Noised Images Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Pixelated Images ///////////////\n",
    "\n",
    "pixelate = lambda x: x.resize((int(32 * 0.2), int(32 * 0.2)), PILImage.BOX).resize((32, 32), PILImage.BOX)\n",
    "ood_loader.dataset.transform = trn.Compose([pixelate, trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "print('\\n\\nPixelate Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// RGB Ghosted/Shifted Images ///////////////\n",
    "\n",
    "rgb_shift = lambda x: torch.cat((x[1:2].index_select(2, torch.LongTensor([i for i in range(32 - 1, -1, -1)])),\n",
    "                                 x[2:, :, :], x[0:1, :, :]), 0)\n",
    "ood_loader.dataset.transform = trn.Compose([trn.ToTensor(), rgb_shift, trn.Normalize(mean, std)])\n",
    "\n",
    "print('\\n\\nRGB Ghosted/Shifted Image Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Inverted Images ///////////////\n",
    "\n",
    "# not done on all channels to make image ood with higher probability\n",
    "invert = lambda x: torch.cat((x[0:1, :, :], 1 - x[1:2, :, ], 1 - x[2:, :, :],), 0)\n",
    "ood_loader.dataset.transform = trn.Compose([trn.ToTensor(), invert, trn.Normalize(mean, std)])\n",
    "\n",
    "print('\\n\\nInverted Image Detection')\n",
    "get_and_print_results(ood_loader)\n",
    "\n",
    "# /////////////// Mean Results ///////////////\n",
    "\n",
    "print('\\n\\nMean Validation Results')\n",
    "print_measures(np.mean(auroc_list), np.mean(aupr_list), np.mean(fpr_list), f, method_name=args['method_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "test_cifar.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
