{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:45:15.973961Z",
     "start_time": "2019-12-27T10:45:15.138719Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UtIKnY967rmO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:45:16.863768Z",
     "start_time": "2019-12-27T10:45:16.849285Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_yvOqAe8ElRF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size = 64,\n",
    "    in_dist_dataset = '20ng',\n",
    "    method='OECC',\n",
    "    save = 'results',\n",
    "    load = 'results', \n",
    "    oe_dataset = 'wikitext2'\n",
    "    )\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "cudnn.benchmark = True  # fire on all cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:45:17.947328Z",
     "start_time": "2019-12-27T10:45:17.702643Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3MCVjwZSy3KD"
   },
   "outputs": [],
   "source": [
    "from utils.display_results import get_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:45:20.308419Z",
     "start_time": "2019-12-27T10:45:18.335235Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aJuvSKyRZamz",
    "outputId": "135aaf20-a010-4a1f-e360-6b094b3c5b60"
   },
   "outputs": [],
   "source": [
    "# ============================ 20 Newsgroups ============================ #\n",
    "TEXT_20ng = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "LABEL_20ng = data.Field(sequential=False)\n",
    "\n",
    "train_20ng = data.TabularDataset(path='20ng-train.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL_20ng),('text', TEXT_20ng)])\n",
    "\n",
    "test_20ng = data.TabularDataset(path='20ng-test.txt',\n",
    "                                 format='csv',\n",
    "                                 fields=[('label', LABEL_20ng),('text', TEXT_20ng)])\n",
    "\n",
    "TEXT_20ng.build_vocab(train_20ng, max_size=10000)\n",
    "LABEL_20ng.build_vocab(train_20ng, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_20ng.vocab))\n",
    "num_classes = len(LABEL_20ng.vocab)\n",
    "print('num labels:', len(LABEL_20ng.vocab))\n",
    "train_iter_20ng = data.BucketIterator(train_20ng, batch_size=args.batch_size, repeat=False)\n",
    "test_iter_20ng = data.BucketIterator(test_20ng, batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ 20 Newsgroups ============================ #\n",
    "\n",
    "ood_num_examples = len(test_iter_20ng.dataset) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_iter_20ng.dataset))\n",
    "recall_level = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:45:48.900829Z",
     "start_time": "2019-12-27T10:45:20.310463Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "id": "R74RQZsnFCxy",
    "outputId": "266356bc-7b9e-4081-9b42-ef2678a8bee2"
   },
   "outputs": [],
   "source": [
    "# ============================ IMDB ============================ #\n",
    "\n",
    "# set up fields\n",
    "TEXT_imdb = data.Field(pad_first=True, lower=True)\n",
    "LABEL_imdb = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n",
    "\n",
    "# build vocab\n",
    "TEXT_imdb.build_vocab(train_20ng.text, max_size=10000)\n",
    "LABEL_imdb.build_vocab(train_imdb, max_size=10000)\n",
    "print('vocab length for IMBD (including special tokens):', len(TEXT_imdb.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n",
    "    (train_imdb, test_imdb), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ IMDB ============================ #\n",
    "\n",
    "# ============================ SNLI ============================ #\n",
    "\n",
    "# set up fields\n",
    "TEXT_snli = data.Field(pad_first=True, lower=True)\n",
    "LABEL_snli = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_snli, val_snli, test_snli = datasets.SNLI.splits(TEXT_snli, LABEL_snli)\n",
    "\n",
    "# build vocab\n",
    "TEXT_snli.build_vocab(train_20ng.text, max_size=10000)\n",
    "LABEL_snli.build_vocab(train_snli, max_size=10000)\n",
    "print('vocab length for SNLI (including special tokens):', len(TEXT_snli.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_snli, val_iter_snli, test_iter_snli = data.BucketIterator.splits(\n",
    "    (train_snli, val_snli, test_snli), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ SNLI ============================ #\n",
    "\n",
    "# ============================ Multi30K ============================ #\n",
    "TEXT_m30k = data.Field(pad_first=True, lower=True)\n",
    "LABEL_m30k = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "data_m30k = datasets.Multi30k.splits(('.en', '.de'),(TEXT_m30k, LABEL_m30k), path='./.data/multi30k/',train=None, validation=None)\n",
    "\n",
    "TEXT_m30k.build_vocab(train_20ng.text, max_size=10000)\n",
    "LABEL_m30k.build_vocab(data_m30k[0], max_size=10000)\n",
    "print('vocab length for Multi30k (including special tokens):', len(TEXT_m30k.vocab))\n",
    "\n",
    "train_iter_m30k = data.BucketIterator(data_m30k[0], batch_size=args.batch_size, repeat=False)\n",
    "# ============================ Multi30K ============================ #\n",
    "\n",
    "# ============================ WMT16 ============================ #\n",
    "TEXT_wmt16 = data.Field(pad_first=True, lower=True)\n",
    "LABEL_wmt16 = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "data_wmt16 = datasets.TranslationDataset.splits(('.en', '.de'),(TEXT_wmt16, LABEL_wmt16), path='./.data/wmt16/',train=None, validation=None)\n",
    "\n",
    "TEXT_wmt16.build_vocab(train_20ng.text, max_size=10000)\n",
    "LABEL_wmt16.build_vocab(data_wmt16[0], max_size=10000)\n",
    "print('vocab length for WMT16 (including special tokens):', len(TEXT_wmt16.vocab))\n",
    "\n",
    "train_iter_wmt16 = data.BucketIterator(data_wmt16[0], batch_size=args.batch_size, repeat=False)\n",
    "# ============================ WMT16 ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Answers) ============================ #\n",
    "\n",
    "TEXT_answers = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/answers-dev.conllu'\n",
    "\n",
    "train_answers = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_answers)))\n",
    "\n",
    "TEXT_answers.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for EWT-Answers (including special tokens):', len(TEXT_answers.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_answers = data.BucketIterator.splits(\n",
    "    (train_answers,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Answers) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Email) ============================ #\n",
    "\n",
    "TEXT_email = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/email-dev.conllu'\n",
    "\n",
    "train_email = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_email)))\n",
    "\n",
    "TEXT_email.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for EWT-Email (including special tokens):', len(TEXT_email.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_email = data.BucketIterator.splits(\n",
    "    (train_email,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Email) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Newsgroup) ============================ #\n",
    "\n",
    "TEXT_newsgroup = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/newsgroup-dev.conllu'\n",
    "\n",
    "train_newsgroup = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_newsgroup)))\n",
    "\n",
    "TEXT_newsgroup.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for EWT-Newsgroup (including special tokens):', len(TEXT_newsgroup.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_newsgroup = data.BucketIterator.splits(\n",
    "    (train_newsgroup,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Newsgroup) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Reviews) ============================ #\n",
    "\n",
    "TEXT_reviews = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/reviews-dev.conllu'\n",
    "\n",
    "train_reviews = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_reviews)))\n",
    "\n",
    "TEXT_reviews.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for EWT-Reviews (including special tokens):', len(TEXT_reviews.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_reviews = data.BucketIterator.splits(\n",
    "    (train_reviews,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Reviews) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Weblog) ============================ #\n",
    "\n",
    "TEXT_weblog = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/reviews-dev.conllu'\n",
    "\n",
    "train_weblog = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_weblog)))\n",
    "\n",
    "TEXT_weblog.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for EWT-Weblog (including special tokens):', len(TEXT_weblog.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_weblog = data.BucketIterator.splits(\n",
    "    (train_weblog,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Weblog) ============================ #\n",
    "\n",
    "# ============================ Yelp Reviews ============================ #\n",
    "TEXT_yelp = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "yelp_data = data.TabularDataset(path='./yelp.csv',\n",
    "                                  format='csv',\n",
    "                                  fields=[(None, None), ('text', TEXT_yelp)],skip_header=True)\n",
    "\n",
    "TEXT_yelp.build_vocab(train_20ng.text, max_size=10000)\n",
    "print('vocab length for Yelp Reviews (including special tokens):', len(TEXT_yelp.vocab))\n",
    "\n",
    "train_iter_yelp = data.BucketIterator(yelp_data, batch_size=args.batch_size, repeat=False)\n",
    "# ============================ Yelp Reviews ============================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:46:21.062056Z",
     "start_time": "2019-12-27T10:45:48.902631Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "41b-WxEkAq6N"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT_20ng.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = ClfGRU(num_classes-1)\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/model_finetune.dict'))  # change location as per our method\n",
    "print('\\nLoaded model.\\n')\n",
    "\n",
    "\n",
    "def get_scores(dataset_iterator, ood=False, translation_dataset = False, snli=False):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    outlier_scores = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n",
    "        if ood and (batch_idx * args.batch_size > ood_num_examples):\n",
    "            break\n",
    "\n",
    "        if snli:\n",
    "            inputs = batch.hypothesis.t()\n",
    "        else:\n",
    "            if translation_dataset:\n",
    "                inputs = batch.src.t()\n",
    "            else:        \n",
    "                inputs = batch.text.t()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n",
    "        msp = -1 * torch.max(smax, dim=1)[0]\n",
    "\n",
    "        # ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)\n",
    "\n",
    "        outlier_scores.extend(list(msp.data.cpu().numpy()))\n",
    "\n",
    "    return outlier_scores\n",
    "\n",
    "\n",
    "\n",
    "# ============================ OECC ============================ #\n",
    "\n",
    "test_scores = get_scores(test_iter_20ng)\n",
    "\n",
    "titles = ['SNLI', 'IMDB', 'Multi30K', 'WMT16', 'English Web Treebank (Answers)',\n",
    "          'English Web Treebank (Email)', 'English Web Treebank (Newsgroup)',\n",
    "          'English Web Treebank (Reviews)', 'English Web Treebank (Weblog)',\n",
    "          'Yelp Reviews']\n",
    "\n",
    "iterators = [test_iter_snli, test_iter_imdb, train_iter_m30k, train_iter_wmt16, train_iter_answers,\n",
    "             train_iter_email, train_iter_newsgroup, train_iter_reviews, train_iter_weblog,\n",
    "             train_iter_yelp]\n",
    "\n",
    "\n",
    "mean_fprs = []\n",
    "mean_aurocs = []\n",
    "mean_auprs = []\n",
    "\n",
    "f = open(f'./{args.save}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/OECC_eval_results.txt', 'w')\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    title = titles[i]\n",
    "    iterator = iterators[i]\n",
    "    \n",
    "    if '30K' in title or '16' in title:\n",
    "        translation_dataset=True\n",
    "    else:\n",
    "        translation_dataset=False\n",
    "\n",
    "    print(f'\\n{title}')\n",
    "    f.write(f'\\n{title}')\n",
    "    fprs, aurocs, auprs = [], [], []\n",
    "    for i in range(10):\n",
    "        ood_scores = get_scores(iterator, ood=True, translation_dataset = translation_dataset, snli=True) if 'SNLI' in title else get_scores(iterator, ood=True, translation_dataset=translation_dataset)\n",
    "        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n",
    "        fprs.append(fpr)\n",
    "        aurocs.append(auroc)\n",
    "        auprs.append(aupr)\n",
    "\n",
    "    print(f'FPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    f.write(f'\\nFPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    print(f'AUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    f.write(f'\\nAUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    print(f'AUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})')\n",
    "    f.write(f'\\nAUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})\\n')\n",
    "\n",
    "    mean_fprs.append(np.mean(fprs))\n",
    "    mean_aurocs.append(np.mean(aurocs))\n",
    "    mean_auprs.append(np.mean(auprs))\n",
    "\n",
    "print()\n",
    "print(f'OOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "print(f'OOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "print(f'OOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s0ES4Qaujc4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eval_OOD_20ng.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
