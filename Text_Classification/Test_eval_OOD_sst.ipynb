{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T23:52:28.810298Z",
     "start_time": "2019-11-03T23:52:27.806178Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UtIKnY967rmO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T23:52:30.834146Z",
     "start_time": "2019-11-03T23:52:30.812551Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_yvOqAe8ElRF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size = 64,\n",
    "    in_dist_dataset = 'sst',\n",
    "    method='OECC',\n",
    "    save = 'results',\n",
    "    load = 'results', \n",
    "    oe_dataset = 'wikitext2'\n",
    "    )\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "cudnn.benchmark = True  # fire on all cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T23:52:33.880671Z",
     "start_time": "2019-11-03T23:52:33.333325Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3MCVjwZSy3KD"
   },
   "outputs": [],
   "source": [
    "from utils.display_results import get_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T23:52:36.515927Z",
     "start_time": "2019-11-03T23:52:33.884084Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5169,
     "status": "ok",
     "timestamp": 1564467797273,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "QgpuEZwtzMV4",
    "outputId": "cec99551-d715-4017-8d4b-9c5a522d7ed4"
   },
   "outputs": [],
   "source": [
    "# ============================ SST ============================ #\n",
    "# set up fields\n",
    "TEXT_sst = data.Field(pad_first=True)\n",
    "LABEL_sst = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_sst, val_sst, test_sst = datasets.SST.splits(\n",
    "    TEXT_sst, LABEL_sst, fine_grained=False, train_subtrees=False,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# build vocab\n",
    "TEXT_sst.build_vocab(train_sst, max_size=10000)\n",
    "LABEL_sst.build_vocab(train_sst, max_size=10000)\n",
    "print('vocab length for SST(including special tokens):', len(TEXT_sst.vocab))\n",
    "num_classes = len(LABEL_sst.vocab)\n",
    "print('num labels:', len(LABEL_sst.vocab))\n",
    "# create our own iterator, avoiding the calls to build_vocab in SST.iters\n",
    "train_iter_sst, val_iter_sst, test_iter_sst = data.BucketIterator.splits(\n",
    "    (train_sst, val_sst, test_sst), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "\n",
    "ood_num_examples = len(test_iter_sst.dataset) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_iter_sst.dataset))\n",
    "recall_level = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T00:35:56.891833Z",
     "start_time": "2019-09-11T00:26:53.155926Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 199
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 997386,
     "status": "ok",
     "timestamp": 1564468795496,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "R74RQZsnFCxy",
    "outputId": "a7fbc5f7-e08c-4fc6-900c-1a8ebd17428d"
   },
   "outputs": [],
   "source": [
    "# ============================ IMDB ============================ #\n",
    "\n",
    "# set up fields\n",
    "TEXT_imdb = data.Field(pad_first=True, lower=True)\n",
    "LABEL_imdb = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_imdb, test_imdb = datasets.IMDB.splits(TEXT_imdb, LABEL_imdb)\n",
    "\n",
    "# build vocab\n",
    "TEXT_imdb.build_vocab(train_sst.text, max_size=10000)\n",
    "LABEL_imdb.build_vocab(train_imdb, max_size=10000)\n",
    "print('vocab length for IMBD (including special tokens):', len(TEXT_imdb.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_imdb, test_iter_imdb = data.BucketIterator.splits(\n",
    "    (train_imdb, test_imdb), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ IMDB ============================ #\n",
    "\n",
    "# ============================ SNLI ============================ #\n",
    "\n",
    "# set up fields\n",
    "TEXT_snli = data.Field(pad_first=True, lower=True)\n",
    "LABEL_snli = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_snli, val_snli, test_snli = datasets.SNLI.splits(TEXT_snli, LABEL_snli)\n",
    "\n",
    "# build vocab\n",
    "TEXT_snli.build_vocab(train_sst.text, max_size=10000)\n",
    "LABEL_snli.build_vocab(train_snli, max_size=10000)\n",
    "print('vocab length for SNLI (including special tokens):', len(TEXT_snli.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_snli, val_iter_snli, test_iter_snli = data.BucketIterator.splits(\n",
    "    (train_snli, val_snli, test_snli), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ SNLI ============================ #\n",
    "\n",
    "# ============================ Multi30K ============================ #\n",
    "TEXT_m30k = data.Field(pad_first=True, lower=True)\n",
    "LABEL_m30k = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "\n",
    "data_m30k = datasets.Multi30k.splits(('.en', '.de'),(TEXT_m30k, LABEL_m30k), path='./.data/multi30k/',train=None, validation=None)\n",
    "\n",
    "TEXT_m30k.build_vocab(train_sst.text, max_size=10000)\n",
    "LABEL_m30k.build_vocab(data_m30k[0], max_size=10000)\n",
    "print('vocab length for Multi30k (including special tokens):', len(TEXT_m30k.vocab))\n",
    "\n",
    "train_iter_m30k = data.BucketIterator(data_m30k[0], batch_size=args.batch_size, repeat=False)\n",
    "# ============================ Multi30K ============================ #\n",
    "\n",
    "# ============================ WMT16 ============================ #\n",
    "TEXT_wmt16 = data.Field(pad_first=True, lower=True)\n",
    "LABEL_wmt16 = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "data_wmt16 = datasets.TranslationDataset.splits(('.en', '.de'),(TEXT_wmt16, LABEL_wmt16), path='./.data/wmt16/',train=None, validation=None)\n",
    "\n",
    "TEXT_wmt16.build_vocab(train_sst.text, max_size=10000)\n",
    "LABEL_wmt16.build_vocab(data_wmt16[0], max_size=10000)\n",
    "print('vocab length for WMT16 (including special tokens):', len(TEXT_wmt16.vocab))\n",
    "\n",
    "train_iter_wmt16 = data.BucketIterator(data_wmt16[0], batch_size=args.batch_size, repeat=False)\n",
    "# ============================ WMT16 ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Answers) ============================ #\n",
    "\n",
    "TEXT_answers = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/answers-dev.conllu'\n",
    "\n",
    "train_answers = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_answers)))\n",
    "\n",
    "TEXT_answers.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for EWT-Answers (including special tokens):', len(TEXT_answers.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_answers = data.BucketIterator.splits(\n",
    "    (train_answers,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Answers) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Email) ============================ #\n",
    "\n",
    "TEXT_email = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/email-dev.conllu'\n",
    "\n",
    "train_email = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_email)))\n",
    "\n",
    "TEXT_email.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for EWT-Email (including special tokens):', len(TEXT_email.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_email = data.BucketIterator.splits(\n",
    "    (train_email,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Email) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Newsgroup) ============================ #\n",
    "\n",
    "TEXT_newsgroup = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/newsgroup-dev.conllu'\n",
    "\n",
    "train_newsgroup = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_newsgroup)))\n",
    "\n",
    "TEXT_newsgroup.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for EWT-Newsgroup (including special tokens):', len(TEXT_newsgroup.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_newsgroup = data.BucketIterator.splits(\n",
    "    (train_newsgroup,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Newsgroup) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Reviews) ============================ #\n",
    "\n",
    "TEXT_reviews = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/reviews-dev.conllu'\n",
    "\n",
    "train_reviews = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_reviews)))\n",
    "\n",
    "TEXT_reviews.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for EWT-Reviews (including special tokens):', len(TEXT_reviews.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_reviews = data.BucketIterator.splits(\n",
    "    (train_reviews,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Reviews) ============================ #\n",
    "\n",
    "# ============================ English Web Treebank (Weblog) ============================ #\n",
    "\n",
    "TEXT_weblog = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "treebank_path = './.data/eng_web_tbk/reviews-dev.conllu'\n",
    "\n",
    "train_weblog = datasets.SequenceTaggingDataset(path=treebank_path, fields=((None, None), ('text', TEXT_weblog)))\n",
    "\n",
    "TEXT_weblog.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for EWT-Weblog (including special tokens):', len(TEXT_weblog.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_weblog = data.BucketIterator.splits(\n",
    "    (train_weblog,), batch_size=args.batch_size, repeat=False)[0]\n",
    "\n",
    "# ============================ English Web Treebank (Weblog) ============================ #\n",
    "\n",
    "# ============================ Yelp Reviews ============================ #\n",
    "TEXT_yelp = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "yelp_data = data.TabularDataset(path='./yelp.csv',\n",
    "                                  format='csv',\n",
    "                                  fields=[(None, None), ('text', TEXT_yelp)],skip_header=True)\n",
    "\n",
    "TEXT_yelp.build_vocab(train_sst.text, max_size=10000)\n",
    "print('vocab length for Yelp Reviews (including special tokens):', len(TEXT_yelp.vocab))\n",
    "\n",
    "train_iter_yelp = data.BucketIterator(yelp_data, batch_size=args.batch_size, repeat=False)\n",
    "# ============================ Yelp Reviews ============================ #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-03T23:53:28.517058Z",
     "start_time": "2019-11-03T23:53:28.489224Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 72
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 996928,
     "status": "ok",
     "timestamp": 1564468811248,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "OLL-xqubzVFF",
    "outputId": "35b5b924-5ea3-42a4-91ce-5e3883f6419b"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT_sst.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "model = ClfGRU(num_classes-1)\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/model_finetune.dict'))  # change location as per our method\n",
    "print('\\nLoaded model.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T00:44:31.262641Z",
     "start_time": "2019-09-11T00:44:08.784009Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 999
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1015250,
     "status": "ok",
     "timestamp": 1564468829981,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "41b-WxEkAq6N",
    "outputId": "b9ad4857-92e9-43ce-9ec1-5a71f313b7ef"
   },
   "outputs": [],
   "source": [
    "def get_scores(dataset_iterator, ood=False, translation_dataset = False, snli=False):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    outlier_scores = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n",
    "        if ood and (batch_idx * args.batch_size > ood_num_examples):\n",
    "            break\n",
    "\n",
    "        if snli:\n",
    "            inputs = batch.hypothesis.t()\n",
    "        else:\n",
    "            if translation_dataset:\n",
    "                inputs = batch.src.t()\n",
    "            else:        \n",
    "                inputs = batch.text.t()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        \n",
    "        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n",
    "        msp = -1 * torch.max(smax, dim=1)[0]\n",
    "      \n",
    "    #      ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)  # negative cross entropy\n",
    "        # test = (F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1) * (1 / torch.FloatTensor([logits.size(1)]).cuda().mean()).log()).sum(1)\n",
    "#         test = -1 * (F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1) * smax).sum(1)\n",
    "\n",
    "        outlier_scores.extend(list(msp.data.numpy()))\n",
    "\n",
    "    return outlier_scores\n",
    "\n",
    "\n",
    "\n",
    "# ============================ OECC ============================ #\n",
    "\n",
    "test_scores = get_scores(test_iter_sst)\n",
    "\n",
    "titles = ['SNLI', 'IMDB', 'Multi30K', 'WMT16', 'English Web Treebank (Answers)',\n",
    "          'English Web Treebank (Email)', 'English Web Treebank (Newsgroup)',\n",
    "          'English Web Treebank (Reviews)', 'English Web Treebank (Weblog)',\n",
    "          'Yelp Reviews']\n",
    "\n",
    "iterators = [test_iter_snli, test_iter_imdb, train_iter_m30k,  train_iter_wmt16, train_iter_answers,\n",
    "             train_iter_email, train_iter_newsgroup, train_iter_reviews, train_iter_weblog,\n",
    "            train_iter_yelp]\n",
    "\n",
    "\n",
    "mean_fprs = []\n",
    "mean_aurocs = []\n",
    "mean_auprs = []\n",
    "\n",
    "f = open(f'./{args.save}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/OECC_eval_results.txt', 'w')\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    title = titles[i]\n",
    "    iterator = iterators[i]\n",
    "    \n",
    "    if '30K' in title or '16' in title:\n",
    "        translation_dataset=True\n",
    "    else:\n",
    "        translation_dataset=False\n",
    "        \n",
    "    print(f'\\n{title}')\n",
    "    f.write(f'\\n{title}')\n",
    "    fprs, aurocs, auprs = [], [], []\n",
    "    for i in range(10):\n",
    "        ood_scores = get_scores(iterator, ood=True, translation_dataset = translation_dataset, snli=True) if 'SNLI' in title else get_scores(iterator, ood=True, translation_dataset=translation_dataset)\n",
    "        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n",
    "        fprs.append(fpr)\n",
    "        aurocs.append(auroc)\n",
    "        auprs.append(aupr)\n",
    "\n",
    "    print(f'FPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    f.write(f'\\nFPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    print(f'AUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    f.write(f'\\nAUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    print(f'AUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})')\n",
    "    f.write(f'\\nAUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})\\n')\n",
    "\n",
    "    mean_fprs.append(np.mean(fprs))\n",
    "    mean_aurocs.append(np.mean(aurocs))\n",
    "    mean_auprs.append(np.mean(auprs))\n",
    "\n",
    "print()\n",
    "print(f'OOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "print(f'OOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "print(f'OOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s0ES4Qaujc4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eval_OOD_sst.ipynb",
   "provenance": [
    {
     "file_id": "1s7IYTjdG40I2GEObKfl4AFgLxncH4zJn",
     "timestamp": 1563772067136
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
