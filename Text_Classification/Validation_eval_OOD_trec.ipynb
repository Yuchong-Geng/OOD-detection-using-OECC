{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:30:16.750564Z",
     "start_time": "2019-11-04T02:30:15.838109Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UtIKnY967rmO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "\n",
    "import pandas as pd\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "import spacy\n",
    "import re\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:30:19.340134Z",
     "start_time": "2019-11-04T02:30:19.324739Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_yvOqAe8ElRF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    batch_size = 64,\n",
    "    in_dist_dataset = 'trec',\n",
    "    method='OECC',\n",
    "    save = 'results',\n",
    "    load = 'results', \n",
    "    oe_dataset = 'wikitext2'\n",
    "    )\n",
    "\n",
    "torch.set_grad_enabled(False)\n",
    "cudnn.benchmark = True  # fire on all cylinders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:30:20.925450Z",
     "start_time": "2019-11-04T02:30:20.685532Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "3MCVjwZSy3KD"
   },
   "outputs": [],
   "source": [
    "from utils.display_results import get_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:30:22.750409Z",
     "start_time": "2019-11-04T02:30:22.573544Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "aJuvSKyRZamz",
    "outputId": "135aaf20-a010-4a1f-e360-6b094b3c5b60"
   },
   "outputs": [],
   "source": [
    "# ============================ TREC ============================ #\n",
    "# set up fields\n",
    "TEXT_trec = data.Field(pad_first=True, lower=True)\n",
    "LABEL_trec = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_trec, test_trec = datasets.TREC.splits(TEXT_trec, LABEL_trec, fine_grained=True)\n",
    "\n",
    "\n",
    "# build vocab\n",
    "TEXT_trec.build_vocab(train_trec, max_size=10000)\n",
    "LABEL_trec.build_vocab(train_trec, max_size=10000)\n",
    "print('vocab length (including special tokens):', len(TEXT_trec.vocab))\n",
    "num_classes = len(LABEL_trec.vocab)\n",
    "print('num labels:', len(LABEL_trec.vocab))\n",
    "\n",
    "# make iterators\n",
    "train_iter_trec, test_iter_trec = data.BucketIterator.splits(\n",
    "    (train_trec, test_trec), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "# ============================ TREC ============================ #\n",
    "\n",
    "ood_num_examples = len(test_iter_trec.dataset) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_iter_trec.dataset))\n",
    "recall_level = 0.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use 20 Newsgroup and SST as validation OOD data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:47:19.973718Z",
     "start_time": "2019-11-04T02:47:16.381598Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================ 20 Newsgroups ============================ #\n",
    "TEXT_20ng = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "LABEL_20ng = data.Field(sequential=False)\n",
    "\n",
    "train_20ng = data.TabularDataset(path='20_newsgroup_train.csv',\n",
    "                                 format='csv',\n",
    "                                 fields=[('text', TEXT_20ng), ('label', LABEL_20ng)])\n",
    "\n",
    "test_20ng = data.TabularDataset(path='20_newsgroup_test.csv',\n",
    "                                 format='csv',\n",
    "                                 fields=[('text', TEXT_20ng), ('label', LABEL_20ng)])\n",
    "\n",
    "TEXT_20ng.build_vocab(train_20ng, max_size=8679)\n",
    "LABEL_20ng.build_vocab(train_20ng, max_size=8679)\n",
    "print('vocab length (including special tokens):', len(TEXT_20ng.vocab))\n",
    "#num_classes = len(LABEL_20ng.vocab)\n",
    "print('num labels:', len(LABEL_20ng.vocab))\n",
    "train_iter_20ng = data.BucketIterator(train_20ng, batch_size=args.batch_size, repeat=False)\n",
    "test_iter_20ng = data.BucketIterator(test_20ng, batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "\n",
    "\n",
    "# ============================ SST ============================ #\n",
    "# set up fields\n",
    "TEXT_sst = data.Field(pad_first=True)\n",
    "LABEL_sst = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_sst, val_sst, test_sst = datasets.SST.splits(\n",
    "    TEXT_sst, LABEL_sst, fine_grained=False, train_subtrees=False,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# build vocab\n",
    "TEXT_sst.build_vocab(train_sst, max_size=8679)\n",
    "LABEL_sst.build_vocab(train_sst, max_size=8679)\n",
    "print('vocab length for SST(including special tokens):', len(TEXT_sst.vocab))\n",
    "#num_classes = len(LABEL_sst.vocab)\n",
    "print('num labels:', len(LABEL_sst.vocab))\n",
    "# create our own iterator, avoiding the calls to build_vocab in SST.iters\n",
    "train_iter_sst, val_iter_sst, test_iter_sst = data.BucketIterator.splits(\n",
    "    (train_sst, val_sst, test_sst), batch_size=args.batch_size, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-04T02:47:23.720431Z",
     "start_time": "2019-11-04T02:47:21.952781Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "41b-WxEkAq6N"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT_trec.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ClfGRU(num_classes-1)\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/model_finetune.dict'))  # change location as per our method\n",
    "print('\\nLoaded model.\\n')\n",
    "\n",
    "\n",
    "def get_scores(dataset_iterator, ood=False, translation_dataset = False, snli=False):\n",
    "    model.eval()\n",
    "    model.cpu()\n",
    "    \n",
    "    outlier_scores = []\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(dataset_iterator)):\n",
    "        if ood and (batch_idx * args.batch_size > ood_num_examples):\n",
    "            break\n",
    "\n",
    "        if snli:\n",
    "            inputs = batch.hypothesis.t()\n",
    "        else:\n",
    "            if translation_dataset:\n",
    "                inputs = batch.src.t()\n",
    "            else:        \n",
    "                inputs = batch.text.t()\n",
    "\n",
    "        logits = model(inputs)\n",
    "        smax = F.softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1)\n",
    "        msp = -1 * torch.max(smax, dim=1)[0]\n",
    "\n",
    "        # ce_to_unif = F.log_softmax(logits - torch.max(logits, dim=1, keepdim=True)[0], dim=1).mean(1)\n",
    "\n",
    "        outlier_scores.extend(list(msp.data.cpu().numpy()))\n",
    "\n",
    "    return outlier_scores\n",
    "\n",
    "\n",
    "\n",
    "# ============================ OECC ============================ #\n",
    "\n",
    "test_scores = get_scores(test_iter_trec)\n",
    "\n",
    "titles = ['20 Newsgroup', 'SST']\n",
    "\n",
    "iterators = [train_iter_20ng, train_iter_sst]\n",
    "\n",
    "\n",
    "mean_fprs = []\n",
    "mean_aurocs = []\n",
    "mean_auprs = []\n",
    "\n",
    "f = open(f'./{args.save}/{args.in_dist_dataset}/{args.method}/{args.oe_dataset}/OECC_eval_results.txt', 'w')\n",
    "\n",
    "for i in range(len(titles)):\n",
    "    title = titles[i]\n",
    "    iterator = iterators[i]\n",
    "    \n",
    "    if '30K' in title or '16' in title:\n",
    "        translation_dataset=True\n",
    "    else:\n",
    "        translation_dataset=False\n",
    "\n",
    "    print(f'\\n{title}')\n",
    "    f.write(f'\\n{title}')\n",
    "    fprs, aurocs, auprs = [], [], []\n",
    "    for i in range(10):\n",
    "        ood_scores = get_scores(iterator, ood=True, translation_dataset = translation_dataset, snli=True) if 'SNLI' in title else get_scores(iterator, ood=True, translation_dataset=translation_dataset)\n",
    "        fpr, auroc, aupr = get_performance(ood_scores, test_scores, expected_ap, recall_level=recall_level)\n",
    "        fprs.append(fpr)\n",
    "        aurocs.append(auroc)\n",
    "        auprs.append(aupr)\n",
    "\n",
    "    print(f'FPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    f.write(f'\\nFPR{int(100 * recall_level):d}:\\t\\t\\t{np.mean(fprs):.4f} ({np.std(fprs):.4f})')\n",
    "    print(f'AUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    f.write(f'\\nAUROC:\\t\\t\\t{np.mean(aurocs):.4f} ({np.std(aurocs):.4f})')\n",
    "    print(f'AUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})')\n",
    "    f.write(f'\\nAUPR:\\t\\t\\t{np.mean(auprs):.4f} ({np.std(auprs):.4f})\\n')\n",
    "\n",
    "    mean_fprs.append(np.mean(fprs))\n",
    "    mean_aurocs.append(np.mean(aurocs))\n",
    "    mean_auprs.append(np.mean(auprs))\n",
    "\n",
    "print()\n",
    "print(f'OOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean FPR: {np.mean(mean_fprs):.4f}')\n",
    "print(f'OOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUROC: {np.mean(mean_aurocs):.4f}')\n",
    "print(f'OOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "f.write(f'\\nOOD dataset mean AUPR: {np.mean(mean_auprs):.4f}')\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4s0ES4Qaujc4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "eval_OOD_20ng.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
