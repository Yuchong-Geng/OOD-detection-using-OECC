{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:34:39.343512Z",
     "start_time": "2019-09-14T19:34:39.289009Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UtIKnY967rmO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:34:40.595467Z",
     "start_time": "2019-09-14T19:34:40.555159Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_yvOqAe8ElRF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    in_dist_dataset='sst', # 'sst', or 'trec', or '20ng'\n",
    "    epochs=5,\n",
    "    batch_size=64,\n",
    "    learning_rate = 0.01,\n",
    "    momentum= 0.5,\n",
    "    test_bs = 256,\n",
    "    save='results',\n",
    "    load= 'results',\n",
    "    test= 'store_true',\n",
    "    mix='store_true',\n",
    "    prefetch= 2,    \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:34:42.551475Z",
     "start_time": "2019-09-14T19:34:41.141947Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4526,
     "status": "ok",
     "timestamp": 1564653946197,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "T-KIZVW98Iqs",
    "outputId": "d1718e64-7280-4334-c4fa-7dec9727b1d2"
   },
   "outputs": [],
   "source": [
    "# root='NLP_classification'\n",
    "if args.in_dist_dataset == 'sst':\n",
    "    # set up fields\n",
    "    TEXT = data.Field(pad_first=True)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    # make splits for data\n",
    "    train, val, test = datasets.SST.splits(\n",
    "        TEXT, LABEL, fine_grained=False, train_subtrees=False,\n",
    "        filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "    # build vocab\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "    # create our own iterator, avoiding the calls to build_vocab in SST.iters\n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, val, test), batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.in_dist_dataset == '20ng':\n",
    "    \n",
    "    TEXT = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    train = data.TabularDataset(path='20ng-train.txt',\n",
    "                                     format='csv',\n",
    "                                     fields=[('label', LABEL),('text', TEXT)])\n",
    "\n",
    "    test = data.TabularDataset(path='20ng-test.txt',\n",
    "                                     format='csv',\n",
    "                                     fields=[('label', LABEL),('text', TEXT)])\n",
    "    \n",
    "\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "    train_iter = data.BucketIterator(train, batch_size=args.batch_size, repeat=False)\n",
    "    test_iter = data.BucketIterator(test, batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.in_dist_dataset == 'trec':\n",
    "    # set up fields\n",
    "    TEXT = data.Field(pad_first=True, lower=True)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    # make splits for data\n",
    "    train, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n",
    "\n",
    "\n",
    "    # build vocab\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "\n",
    "    # make iterators\n",
    "    train_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, test), batch_size=args.batch_size, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:34:46.350923Z",
     "start_time": "2019-09-14T19:34:42.553891Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hEgWpu7fks0v"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n",
    "            bias=True, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "model = ClfGRU(num_classes-1).cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-11T00:56:11.751340Z",
     "start_time": "2019-09-11T00:56:05.465535Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 31699,
     "status": "ok",
     "timestamp": 1564653990076,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "Q3p-oJzA8e3T",
    "outputId": "2b241fec-0424-4d0e-8d92-72dc8e8570c4"
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(args.save, args.in_dist_dataset+ '/baseline/baseline_training_results.txt'), 'w')\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    loss_ema = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(train_iter)):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label -1\n",
    "        \n",
    "        inputs = inputs.cuda()\n",
    "        labels = labels.cuda()\n",
    "        \n",
    "        logits = model(inputs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_ema = loss_ema * 0.9 + loss.data.cpu().numpy() * 0.1\n",
    "\n",
    "        if batch_idx % 200 == 0:\n",
    "            print(f'iter: {batch_idx} | loss_ema: {loss_ema}')\n",
    "            f.write(f'\\niter: {batch_idx} | loss_ema: {loss_ema:.4f} | ')\n",
    "                                   \n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    num_examples = 0\n",
    "    correct = 0\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(test_iter)):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label - 1\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, size_average=False)\n",
    "        running_loss += loss.data.cpu().numpy()\n",
    "\n",
    "        pred = logits.max(1)[1]\n",
    "        correct += pred.eq(labels).sum().data.cpu().numpy()\n",
    "\n",
    "        num_examples += inputs.shape[0]\n",
    "\n",
    "    acc = correct / num_examples\n",
    "    loss = running_loss / num_examples\n",
    "\n",
    "    return acc, loss\n",
    "\n",
    "                       \n",
    "for epoch in range(args.epochs):\n",
    "    print('Epoch', epoch)\n",
    "    f.write(f'\\nEpoch {epoch}')\n",
    "    train()\n",
    "    acc, loss = evaluate()\n",
    "    print(f'test acc: {acc:.3f} | test loss: {loss:.4f}\\n')\n",
    "    f.write(f'test acc: {acc:.3f} | test loss: {loss:.4f}\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "torch.save(model.state_dict(), f'./{args.save}/{args.in_dist_dataset}/baseline/model.dict')\n",
    "print('Saved model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:34:46.614701Z",
     "start_time": "2019-09-14T19:34:46.353132Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "R74RQZsnFCxy"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n",
    "            bias=True, batch_first=True,bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ClfGRU(num_classes-1).cuda()  # change to match dataset\n",
    "\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/baseline/model.dict'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:39:57.941100Z",
     "start_time": "2019-09-14T19:39:57.892720Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    num_examples = 0\n",
    "    correct = 0\n",
    "    acc_bm = []\n",
    "    conf_bm= []\n",
    "    for batch_idx, batch in enumerate(iter(test_iter)):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label - 1\n",
    "        \n",
    "        inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, size_average=False)\n",
    "        running_loss += loss.data.cpu().numpy()\n",
    "\n",
    "        pred = logits.max(1)[1]\n",
    "        runnning_acc = pred.eq(labels).sum().data.cpu().numpy()\n",
    "        correct += pred.eq(labels).sum().data.cpu().numpy()\n",
    "\n",
    "        num_examples += inputs.shape[0]\n",
    "        \n",
    "        # avg confidence\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf = torch.max(probs,dim=1).values.sum().item()\n",
    "        \n",
    "        acc_bm.append(runnning_acc/logits.shape[0])\n",
    "        conf_bm.append(conf/logits.shape[0])\n",
    "\n",
    "    acc = correct / num_examples\n",
    "    loss = running_loss / num_examples\n",
    "\n",
    "    return acc_bm, conf_bm, 1-acc, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:39:58.383244Z",
     "start_time": "2019-09-14T19:39:58.278109Z"
    }
   },
   "outputs": [],
   "source": [
    "acc, conf, err, n = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:39:16.006930Z",
     "start_time": "2019-09-14T19:39:15.955751Z"
    }
   },
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:40:01.384640Z",
     "start_time": "2019-09-14T19:40:01.341106Z"
    }
   },
   "outputs": [],
   "source": [
    "bm = test_iter.batch_size\n",
    "ece = (bm/n)*np.abs(np.subtract(acc, conf)).sum()\n",
    "ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:40:03.195448Z",
     "start_time": "2019-09-14T19:40:03.148767Z"
    }
   },
   "outputs": [],
   "source": [
    "mce =  max(np.abs(np.subtract(acc, conf))) \n",
    "mce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
