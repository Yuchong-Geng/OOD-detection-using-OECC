{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:28.747835Z",
     "start_time": "2019-12-27T10:44:28.303522Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "UtIKnY967rmO"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import pickle\n",
    "import argparse\n",
    "import math\n",
    "import time\n",
    "from bisect import bisect_left\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable as V\n",
    "import torchtext\n",
    "\n",
    "from torchtext import data\n",
    "from torchtext import datasets\n",
    "\n",
    "import csv\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "\n",
    "import tqdm\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:40.407573Z",
     "start_time": "2019-12-27T10:44:40.392520Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "_yvOqAe8ElRF"
   },
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    in_dist_dataset = '20ng', # 'sst' or 'trec' or '20ng'\n",
    "    oe_dataset = 'wikitext2',\n",
    "    epochs = 2, # Fine-tune epochs\n",
    "    batch_size = 64,\n",
    "    learning_rate = 0.01,\n",
    "    momentum = 0.5,\n",
    "    test_bs = 256,\n",
    "    save = 'results',\n",
    "    load = 'results',\n",
    "    test = 'store_true',\n",
    "    mix ='store_true',\n",
    "    ngpu= 1,\n",
    "    prefetch= 2,\n",
    "    lambda_1 = 0.1,\n",
    "    lambda_2 = 0.05\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:44.313669Z",
     "start_time": "2019-12-27T10:44:42.387303Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4392,
     "status": "ok",
     "timestamp": 1564644864549,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "T-KIZVW98Iqs",
    "outputId": "6992f54e-d040-4911-c80f-c99bb4900ce2"
   },
   "outputs": [],
   "source": [
    "if args.in_dist_dataset == 'sst':\n",
    "    # set up fields\n",
    "    TEXT = data.Field(pad_first=True)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    # make splits for data\n",
    "    train, val, test = datasets.SST.splits(\n",
    "        TEXT, LABEL, fine_grained=False, train_subtrees=False,\n",
    "        filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "    # build vocab\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "    # create our own iterator, avoiding the calls to build_vocab in SST.iters\n",
    "    train_iter, val_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, val, test), batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.in_dist_dataset == '20ng':\n",
    "    \n",
    "    TEXT = data.Field(pad_first=True, lower=True, fix_length=100)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    train = data.TabularDataset(path='20ng-train.txt',\n",
    "                                     format='csv',\n",
    "                                     fields=[('label', LABEL),('text', TEXT)])\n",
    "\n",
    "    test = data.TabularDataset(path='20ng-test.txt',\n",
    "                                     format='csv',\n",
    "                                     fields=[('label', LABEL),('text', TEXT)])\n",
    "    \n",
    "\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "    train_iter = data.BucketIterator(train, batch_size=args.batch_size, repeat=False)\n",
    "    test_iter = data.BucketIterator(test, batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.in_dist_dataset == 'trec':\n",
    "    # set up fields\n",
    "    TEXT = data.Field(pad_first=True, lower=True)\n",
    "    LABEL = data.Field(sequential=False)\n",
    "\n",
    "    # make splits for data\n",
    "    train, test = datasets.TREC.splits(TEXT, LABEL, fine_grained=True)\n",
    "\n",
    "\n",
    "    # build vocab\n",
    "    TEXT.build_vocab(train, max_size=10000)\n",
    "    LABEL.build_vocab(train, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT.vocab))\n",
    "    num_classes = len(LABEL.vocab)\n",
    "    print('num labels:', len(LABEL.vocab))\n",
    "\n",
    "    # make iterators\n",
    "    train_iter, test_iter = data.BucketIterator.splits(\n",
    "        (train, test), batch_size=args.batch_size, repeat=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:45.945793Z",
     "start_time": "2019-12-27T10:44:44.503510Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3799,
     "status": "ok",
     "timestamp": 1564644879031,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "tlgNrggcO9hn",
    "outputId": "6f557fb1-32e4-4dbe-9f8a-44e3a4f64b49"
   },
   "outputs": [],
   "source": [
    "if args.oe_dataset == 'wikitext2':\n",
    "    TEXT_custom = data.Field(pad_first=True, lower=True)\n",
    "    \n",
    "    custom_data = data.TabularDataset(path='./wikitext_sentences',\n",
    "                                      format='csv',\n",
    "                                      fields=[('text', TEXT_custom)])\n",
    "\n",
    "    TEXT_custom.build_vocab(train.text, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT_custom.vocab))\n",
    "\n",
    "    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.oe_dataset == 'wikitext103':\n",
    "    TEXT_custom = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "    custom_data = data.TabularDataset(path='./wikitext103_sentences',\n",
    "                                      format='csv',\n",
    "                                      fields=[('text', TEXT_custom)])\n",
    "\n",
    "    TEXT_custom.build_vocab(train.text, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT_custom.vocab))\n",
    "\n",
    "    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)\n",
    "    \n",
    "elif args.oe_dataset == 'gutenberg':\n",
    "    TEXT_custom = data.Field(pad_first=True, lower=True)\n",
    "\n",
    "    custom_data = data.TabularDataset(path='./.data/gutenberg/gutenberg_sentences',\n",
    "                                      format='csv',\n",
    "                                      fields=[('text', TEXT_custom)])\n",
    "\n",
    "    TEXT_custom.build_vocab(train.text, max_size=10000)\n",
    "    print('vocab length (including special tokens):', len(TEXT_custom.vocab))\n",
    "\n",
    "    train_iter_oe = data.BucketIterator(custom_data, batch_size=args.batch_size, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:50.633410Z",
     "start_time": "2019-12-27T10:44:45.974828Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "hEgWpu7fks0v"
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2,\n",
    "            bias=True, batch_first=True,bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "if args.in_dist_dataset == '20ng':\n",
    "    model = ClfGRU(num_classes-1).cuda()  # change to match dataset\n",
    "else:\n",
    "    model = ClfGRU(num_classes-1).cuda()\n",
    "\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/baseline/model.dict'))\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-27T10:44:57.529617Z",
     "start_time": "2019-12-27T10:44:50.636021Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15530,
     "status": "ok",
     "timestamp": 1564645010819,
     "user": {
      "displayName": "Aristotelis - Angelos Papadopoulos",
      "photoUrl": "https://lh6.googleusercontent.com/-irX6pXaWHWk/AAAAAAAAAAI/AAAAAAAAAB4/zcuYCe7Bl38/s64/photo.jpg",
      "userId": "12133508143730271082"
     },
     "user_tz": 420
    },
    "id": "Q3p-oJzA8e3T",
    "outputId": "0a2c8362-dcee-462a-ec69-05199106c93e"
   },
   "outputs": [],
   "source": [
    "f = open(os.path.join(args.save, args.in_dist_dataset+ f'/OECC/{args.oe_dataset}/OECC_training_results.txt'), 'w')\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    data_loss_ema = 0\n",
    "    oe_loss_ema = 0\n",
    "\n",
    "    for batch_idx, (batch, batch_oe) in enumerate(zip(iter(train_iter), iter(train_iter_oe))):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label - 1 \n",
    "        inputs = inputs.cuda() # To convert to cuda\n",
    "        labels = labels.cuda(non_blocking=True) # To convert to cuda\n",
    "        logits = model(inputs)\n",
    "        data_loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        inputs_oe = batch_oe.text.t()\n",
    "        inputs_oe = inputs_oe.cuda() # To convert to cuda\n",
    "        logits_oe = model(inputs_oe)\n",
    "        \n",
    "        \n",
    "        # OECC LOSS \n",
    "        if args.in_dist_dataset == 'sst':\n",
    "            A_tr = 0.778\n",
    "        elif args.in_dist_dataset == '20ng':\n",
    "            A_tr = 0.739\n",
    "        elif args.in_dist_dataset == 'trec':\n",
    "            A_tr = 0.778\n",
    "        sm = torch.nn.Softmax(dim=1) # Create a Softmax \n",
    "        probabilities = sm(logits) # Get the probabilites for In data only\n",
    "        probabilities_oe = sm(logits_oe) # Get the probabilites for Outliers only\n",
    "        max_probs, _ = torch.max(probabilities, dim=1) # Take the maximum probabilities produced by softmax for In data only\n",
    "        max_probs_oe, _ = torch.max(probabilities_oe, dim=1) # Take the maximum probabilities produced by softmax for Outliers only\n",
    "        prob_diff_in = max_probs - A_tr # Use the training accuracy\n",
    "        data_loss += args.lambda_1 * torch.sum(prob_diff_in**2) ## 1st Regularization term\n",
    "        prob_diff_out = probabilities_oe - (1/(num_classes-1))\n",
    "        oe_loss = args.lambda_2 * torch.sum(torch.abs(prob_diff_out)) ## 2nd Regularization term\n",
    "\n",
    "        loss = data_loss + oe_loss\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        data_loss_ema = data_loss_ema * 0.9 + data_loss.data.cpu().numpy() * 0.1\n",
    "        oe_loss_ema = oe_loss_ema * 0.9 + oe_loss.data.cpu().numpy() * 0.1\n",
    "\n",
    "        if (batch_idx % 200 == 0 or batch_idx < 10):\n",
    "            print('iter: {} \\t| data_loss_ema: {} \\t| oe_loss_ema: {}'.format(\n",
    "                batch_idx, data_loss_ema, oe_loss_ema))\n",
    "            f.write(f'\\niter: {batch_idx} | data_loss_ema: {data_loss_ema:.4f} | oe_loss_ema: {oe_loss_ema:.4f}')\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "def evaluate():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    num_examples = 0\n",
    "    correct = 0\n",
    "    conf = 0.0\n",
    "\n",
    "    for batch_idx, batch in enumerate(iter(test_iter)):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label  - 1 \n",
    "        inputs = inputs.cuda() # To convert to cuda\n",
    "        labels = labels.cuda(non_blocking=True) # To convert to cuda\n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, size_average=False)\n",
    "        running_loss += loss.data.cpu().numpy()\n",
    "\n",
    "        pred = logits.max(1)[1]\n",
    "        correct += pred.eq(labels).sum().data.cpu().numpy()\n",
    "\n",
    "        num_examples += inputs.shape[0]\n",
    "        \n",
    "        # avg confidence\n",
    "        probs =F.softmax(logits, dim = 1)\n",
    "        conf += torch.mean(torch.max(probs,1).values)\n",
    "\n",
    "    acc = correct / num_examples\n",
    "    loss = running_loss / num_examples\n",
    "    avg_conf = conf / num_examples\n",
    "\n",
    "    return acc, loss, avg_conf \n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "    print('Epoch', epoch)\n",
    "    f.write(f'\\nEpoch {epoch}')\n",
    "    train()\n",
    "    acc, loss, conf = evaluate()\n",
    "    print(f'test acc: {acc:.3f} | test loss: {loss:.4f} | avg conf: {conf:.3f}\\n')\n",
    "    f.write(f'\\ntest acc: {acc:.3f} | test loss: {loss:.4f}\\n')\n",
    "\n",
    "f.close()\n",
    "\n",
    "torch.save(model.state_dict(), f'./{args.save}/{args.in_dist_dataset}/OECC/{args.oe_dataset}/model_finetune.dict')\n",
    "print('Saved model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:45:28.520951Z",
     "start_time": "2019-09-14T19:45:27.116164Z"
    }
   },
   "outputs": [],
   "source": [
    "# ============================ SST ============================ #\n",
    "# set up fields\n",
    "TEXT_sst = data.Field(pad_first=True)\n",
    "LABEL_sst = data.Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train_sst, val_sst, test_sst = datasets.SST.splits(\n",
    "    TEXT_sst, LABEL_sst, fine_grained=False, train_subtrees=False,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "# build vocab\n",
    "TEXT_sst.build_vocab(train_sst, max_size=10000)\n",
    "LABEL_sst.build_vocab(train_sst, max_size=10000)\n",
    "print('vocab length for SST(including special tokens):', len(TEXT_sst.vocab))\n",
    "num_classes = len(LABEL_sst.vocab)\n",
    "print('num labels:', len(LABEL_sst.vocab))\n",
    "# create our own iterator, avoiding the calls to build_vocab in SST.iters\n",
    "train_iter_sst, val_iter_sst, test_iter_sst = data.BucketIterator.splits(\n",
    "    (train_sst, val_sst, test_sst), batch_size=args.batch_size, repeat=False)\n",
    "\n",
    "\n",
    "ood_num_examples = len(test_iter_sst.dataset) // 5\n",
    "expected_ap = ood_num_examples / (ood_num_examples + len(test_iter_sst.dataset))\n",
    "recall_level = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:46:05.563751Z",
     "start_time": "2019-09-14T19:46:05.335278Z"
    }
   },
   "outputs": [],
   "source": [
    "class ClfGRU(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(len(TEXT_sst.vocab), 50, padding_idx=1)\n",
    "        self.gru = nn.GRU(input_size=50, hidden_size=128, num_layers=2, bias=True, batch_first=True, bidirectional=False)\n",
    "        self.linear = nn.Linear(128, num_classes)\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        hidden = self.gru(embeds)[1][1]  # select h_n, and select the 2nd layer\n",
    "        logits = self.linear(hidden)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = ClfGRU(num_classes-1)\n",
    "model.load_state_dict(torch.load(f'./{args.load}/{args.in_dist_dataset}/OECC/{args.oe_dataset}/model_finetune.dict'))  # change location as per our method\n",
    "print('\\nLoaded model.\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:47:38.684022Z",
     "start_time": "2019-09-14T19:47:38.648431Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "HPwAhVKu3NCS"
   },
   "outputs": [],
   "source": [
    "def evaluate():\n",
    "    model.eval()\n",
    "    running_loss = 0\n",
    "    num_examples = 0\n",
    "    correct = 0\n",
    "    acc_bm = []\n",
    "    conf_bm= []\n",
    "    for batch_idx, batch in enumerate(iter(test_iter_sst)):\n",
    "        inputs = batch.text.t()\n",
    "        labels = batch.label - 1\n",
    "        \n",
    "#         inputs, labels = inputs.cuda(), labels.cuda()\n",
    "    \n",
    "        logits = model(inputs)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels, size_average=False)\n",
    "        running_loss += loss.data.cpu().numpy()\n",
    "\n",
    "        pred = logits.max(1)[1]\n",
    "        runnning_acc = pred.eq(labels).sum().data.cpu().numpy()\n",
    "        correct += pred.eq(labels).sum().data.cpu().numpy()\n",
    "\n",
    "        num_examples += inputs.shape[0]\n",
    "        \n",
    "        # avg confidence\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        conf = torch.max(probs,dim=1).values.sum().item()\n",
    "        \n",
    "        acc_bm.append(runnning_acc/logits.shape[0])\n",
    "        conf_bm.append(conf/logits.shape[0])\n",
    "\n",
    "    acc = correct / num_examples\n",
    "    loss = running_loss / num_examples\n",
    "\n",
    "    return acc_bm, conf_bm, 1-acc, num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:47:40.739296Z",
     "start_time": "2019-09-14T19:47:40.166310Z"
    }
   },
   "outputs": [],
   "source": [
    "acc, conf, err, n = evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:47:41.844693Z",
     "start_time": "2019-09-14T19:47:41.807690Z"
    }
   },
   "outputs": [],
   "source": [
    "err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:47:44.846772Z",
     "start_time": "2019-09-14T19:47:44.810772Z"
    }
   },
   "outputs": [],
   "source": [
    "bm = test_iter.batch_size\n",
    "ece = (bm/n)*np.abs(np.subtract(acc, conf)).sum()\n",
    "ece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-14T19:47:45.848559Z",
     "start_time": "2019-09-14T19:47:45.812151Z"
    }
   },
   "outputs": [],
   "source": [
    "mce =  max(np.abs(np.subtract(acc, conf))) \n",
    "mce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "train_OUR_METHOD_oe.ipynb",
   "provenance": [
    {
     "file_id": "1s7IYTjdG40I2GEObKfl4AFgLxncH4zJn",
     "timestamp": 1563772067136
    }
   ],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
