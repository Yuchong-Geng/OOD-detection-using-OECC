{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T08:12:13.773763Z",
     "start_time": "2019-12-03T08:12:13.761080Z"
    }
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T08:12:14.872112Z",
     "start_time": "2019-12-03T08:12:14.297833Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import data_loader\n",
    "import numpy as np\n",
    "import calculate_log as callog\n",
    "import models\n",
    "import os\n",
    "import lib_generation\n",
    "\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "\n",
    "parser = argparse.ArgumentParser(description='PyTorch code: Mahalanobis detector')\n",
    "parser.add_argument('--batch_size', type=int, default=200, metavar='N', help='batch size for data loader')\n",
    "parser.add_argument('--dataset', default='cifar10', help='cifar10 | cifar100 | svhn')\n",
    "parser.add_argument('--dataroot', default='./data', help='path to dataset')\n",
    "parser.add_argument('--outf', default='./output/', help='folder to output results')\n",
    "parser.add_argument('--num_classes', type=int, default=10, help='the # of classes')\n",
    "parser.add_argument('--net_type', default='resnet', help='resnet')\n",
    "parser.add_argument('--gpu', type=int, default=0, help='gpu index')\n",
    "args = parser.parse_known_args()[0]\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-03T09:15:56.339346Z",
     "start_time": "2019-12-03T08:12:19.874179Z"
    }
   },
   "outputs": [],
   "source": [
    "# set the path to the Mahal oe_tune model and output\n",
    "pre_trained_net = './results/Mahal_OECC_tune/' + 'cifar10_ResNet34_Mahal_OECC_tune_epoch_29.pth'\n",
    "args.outf = args.outf + args.net_type + '_' + args.dataset + '/'\n",
    "\n",
    "if os.path.isdir(args.outf) == False:\n",
    "    os.mkdir(args.outf)\n",
    "    \n",
    "torch.cuda.manual_seed(0)\n",
    "device = torch.device('cuda:0')    \n",
    "# check the in-distribution dataset\n",
    "if args.dataset == 'cifar100':\n",
    "    args.num_classes = 100\n",
    "if args.dataset == 'svhn':\n",
    "    out_dist_list = ['cifar10', 'imagenet_resize', 'lsun_resize']\n",
    "else:\n",
    "    out_dist_list = ['svhn', 'imagenet_resize', 'lsun_resize']\n",
    "\n",
    "# load networks\n",
    "if args.net_type == 'densenet':\n",
    "    if args.dataset == 'svhn':\n",
    "        model = models.DenseNet3(100, int(args.num_classes))\n",
    "        model.load_state_dict(torch.load(pre_trained_net, map_location = \"cuda:\" + str(args.gpu)))\n",
    "    else:\n",
    "        model = models.DenseNet3(100, int(args.num_classes))\n",
    "        model.load_state_dict(torch.load(pre_trained_net, map_location = \"cuda:\" + str(args.gpu)))\n",
    "    in_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((125.3/255, 123.0/255, 113.9/255), (63.0/255, 62.1/255.0, 66.7/255.0)),])\n",
    "elif args.net_type == 'resnet':\n",
    "    model = models.ResNet34(num_c=args.num_classes)\n",
    "#     model = torch.nn.DataParallel(model, [0,1]).cuda()\n",
    "    model.load_state_dict(torch.load(pre_trained_net))#, map_location = \"cuda:\" + str(args.gpu)))\n",
    "    in_transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),])\n",
    "\n",
    "# model.cuda()\n",
    "model = torch.nn.DataParallel(model, [0,1]).cuda()\n",
    "print('load model: ' + args.net_type)\n",
    "\n",
    "# load dataset\n",
    "print('load target data: ', args.dataset)\n",
    "train_loader, test_loader = data_loader.getTargetDataSet(args.dataset, args.batch_size, in_transform, args.dataroot)\n",
    "\n",
    "# set information about feature extaction\n",
    "model.eval()\n",
    "temp_x = torch.rand(2,3,32,32).cuda(device)\n",
    "temp_x = Variable(temp_x)\n",
    "temp_list = model.module.feature_list(temp_x)[1]\n",
    "num_output = len(temp_list)\n",
    "feature_list = np.empty(num_output)\n",
    "count = 0\n",
    "for out in temp_list:\n",
    "    feature_list[count] = out.size(1)\n",
    "    count += 1\n",
    "\n",
    "print('get sample mean and covariance')\n",
    "sample_mean, precision = lib_generation.sample_estimator(model, args.num_classes, feature_list, train_loader)\n",
    "\n",
    "print('get Mahalanobis scores')\n",
    "m_list = [0.0, 0.01, 0.005, 0.002, 0.0014, 0.001, 0.0005] \n",
    "#m_list = [0, 0.0005, 0.001, 0.0014, 0.002, 0.0024, 0.005, 0.01, 0.05, 0.1, 0.2] \n",
    "for magnitude in m_list:\n",
    "    print('Noise: ' + str(magnitude))\n",
    "    for i in range(num_output):\n",
    "        M_in = lib_generation.get_Mahalanobis_score(model, test_loader, args.num_classes, args.outf, \\\n",
    "                                                    True, args.net_type, sample_mean, precision, i, magnitude)\n",
    "        M_in = np.asarray(M_in, dtype=np.float32)\n",
    "        if i == 0:\n",
    "            Mahalanobis_in = M_in.reshape((M_in.shape[0], -1))\n",
    "        else:\n",
    "            Mahalanobis_in = np.concatenate((Mahalanobis_in, M_in.reshape((M_in.shape[0], -1))), axis=1)\n",
    "\n",
    "    for out_dist in out_dist_list:\n",
    "        out_test_loader = data_loader.getNonTargetDataSet(out_dist, args.batch_size, in_transform, args.dataroot)\n",
    "        print('Out-distribution: ' + out_dist) \n",
    "        for i in range(num_output):\n",
    "            M_out = lib_generation.get_Mahalanobis_score(model, out_test_loader, args.num_classes, args.outf, \\\n",
    "                                                         False, args.net_type, sample_mean, precision, i, magnitude)\n",
    "            M_out = np.asarray(M_out, dtype=np.float32)\n",
    "            if i == 0:\n",
    "                Mahalanobis_out = M_out.reshape((M_out.shape[0], -1))\n",
    "            else:\n",
    "                Mahalanobis_out = np.concatenate((Mahalanobis_out, M_out.reshape((M_out.shape[0], -1))), axis=1)\n",
    "\n",
    "        Mahalanobis_in = np.asarray(Mahalanobis_in, dtype=np.float32)\n",
    "        Mahalanobis_out = np.asarray(Mahalanobis_out, dtype=np.float32)\n",
    "        Mahalanobis_data, Mahalanobis_labels = lib_generation.merge_and_generate_labels(Mahalanobis_out, Mahalanobis_in)\n",
    "        file_name = os.path.join(args.outf, 'Mahalanobis_%s_%s_%s.npy' % (str(magnitude), args.dataset , out_dist))\n",
    "        Mahalanobis_data = np.concatenate((Mahalanobis_data, Mahalanobis_labels), axis=1)\n",
    "        np.save(file_name, Mahalanobis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
