{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:15:53.476006Z",
     "start_time": "2019-12-18T12:15:49.639388Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "NmsBla-3MVdu"
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import pickle\n",
    "import argparse\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torchvision.transforms as trn\n",
    "import torchvision.datasets as dset\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm_notebook, tqdm\n",
    "from models import resnet\n",
    "from utils.tinyimages_80mn_loader import TinyImages \n",
    "from utils.validation_dataset import validation_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:15:57.618071Z",
     "start_time": "2019-12-18T12:15:57.607209Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0emMkqH7UQ4G"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "        'dataset': 'cifar10', # Change to 'cifar100' to run experiments with the CIFAR 100\n",
    "        'model': 'ResNet',  \n",
    "        'calibration': '',\n",
    "        'epochs': 30, #30 for CIFAR 10. 20 for CIFAR 100\n",
    "        'learning_rate': 0.001,\n",
    "        'batch_size': 128,\n",
    "        'oe_batch_size': 256,\n",
    "        'test_bs': 200,\n",
    "        'momentum': 0.9,\n",
    "        'decay': 0.0005, # Weight decay (L2 penalty)\n",
    "        'save': './Mahalanobis_Experiments/results/Mahal_OECC_tune',\n",
    "        'test': 'store_true',\n",
    "        'ngpu': 2,\n",
    "        'prefetch': 4,  \n",
    "        'lambda_1': 0.12, ## To push the known classes to higher prediction probabilities\n",
    "        'lambda_2': 0.1  ## To push the outliers towards uniform\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:15:59.529551Z",
     "start_time": "2019-12-18T12:15:59.473774Z"
    }
   },
   "outputs": [],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:16:09.043604Z",
     "start_time": "2019-12-18T12:16:05.909976Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3973,
     "status": "ok",
     "timestamp": 1563642207114,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "Qfq4pjcokUcA",
    "outputId": "fbda4010-a33a-4c8e-a2ec-82f558cd54c4"
   },
   "outputs": [],
   "source": [
    "root_dir = 'CIFAR'\n",
    "\n",
    "state = {k: v for k, v in args.items()}\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "# mean and standard deviation of channels of CIFAR-10 and CIFAR-100 images\n",
    "mean = [x / 255 for x in [125.3, 123.0, 113.9]]\n",
    "std = [x / 255 for x in [63.0, 62.1, 66.7]]\n",
    "\n",
    "train_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "test_transform = trn.Compose([trn.ToTensor(), trn.Normalize(mean, std)])\n",
    "\n",
    "if args['dataset'] == 'cifar10':\n",
    "    train_data_in = dset.CIFAR10(root_dir, train=True, download=True, transform=train_transform)\n",
    "    test_data = dset.CIFAR10(root_dir, train=False, download=True, transform=test_transform)\n",
    "    num_classes = 10\n",
    "elif args['dataset'] == 'cifar100':\n",
    "    train_data_in = dset.CIFAR100(root_dir, train=True, download=True, transform=train_transform)\n",
    "    test_data = dset.CIFAR100(root_dir, train=False, download=True, transform=test_transform)\n",
    "    num_classes = 100\n",
    "    \n",
    "    \n",
    "calib_indicator = ''\n",
    "if args['calibration']:\n",
    "    train_data_in, val_data = validation_split(train_data_in, val_share=0.1)\n",
    "    calib_indicator = '_calib'\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    batch_size=args['batch_size'], shuffle=False,\n",
    "    num_workers=args['prefetch'], pin_memory=True)\n",
    "    \n",
    "    \n",
    "ood_data = TinyImages(transform=trn.Compose(\n",
    "    [trn.ToTensor(), trn.ToPILImage(), trn.RandomCrop(32, padding=4),\n",
    "     trn.RandomHorizontalFlip(), trn.ToTensor(), trn.Normalize(mean, std)]))\n",
    "\n",
    "\n",
    "# Data Loaders\n",
    "train_loader_in = torch.utils.data.DataLoader(\n",
    "    train_data_in,\n",
    "    batch_size=args['batch_size'], shuffle=True,\n",
    "    num_workers=args['prefetch'], pin_memory=True)\n",
    "\n",
    "train_loader_out = torch.utils.data.DataLoader(\n",
    "    ood_data,\n",
    "    batch_size=args['oe_batch_size'], shuffle=False,\n",
    "    num_workers=args['prefetch'], pin_memory=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    batch_size=args['batch_size'], shuffle=False,\n",
    "    num_workers=args['prefetch'], pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:16:20.066336Z",
     "start_time": "2019-12-18T12:16:13.249137Z"
    }
   },
   "outputs": [],
   "source": [
    "net = resnet.ResNet34(num_classes)\n",
    "\n",
    "net.load_state_dict(torch.load('./Mahalanobis_Experiments/pre_trained/resnet_cifar10.pth'))\n",
    "net = torch.nn.DataParallel(net,[0,1]).cuda()\n",
    "\n",
    "device = torch.device('cuda:0')    \n",
    "net=net.cuda(device)\n",
    "cudnn.benchmark = True  # fire on all cylinders\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.SGD(\n",
    "    net.parameters(), state['learning_rate'], momentum=state['momentum'],\n",
    "    weight_decay=state['decay'], nesterov=True)\n",
    "\n",
    "# Learning Rate\n",
    "def cosine_annealing(step, total_steps, lr_max, lr_min):\n",
    "    return lr_min + (lr_max - lr_min) * 0.5 * (\n",
    "            1 + np.cos(step / total_steps * np.pi))\n",
    "\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer,\n",
    "    lr_lambda=lambda step: cosine_annealing(\n",
    "        step,\n",
    "        args['epochs'] * len(train_loader_in),\n",
    "        1,  # since lr_lambda computes multiplicative factor\n",
    "        1e-6 / args['learning_rate']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-18T12:17:29.367067Z",
     "start_time": "2019-12-18T12:17:03.376892Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 610
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 155996,
     "status": "error",
     "timestamp": 1563642376018,
     "user": {
      "displayName": "Nazim Shaikh",
      "photoUrl": "",
      "userId": "00220105759672556480"
     },
     "user_tz": 420
    },
    "id": "ZgtWI5jKpvPX",
    "outputId": "db791f8f-977d-4e18-8f91-31ef163584b4"
   },
   "outputs": [],
   "source": [
    "# /////////////// Training ///////////////\n",
    "def train():\n",
    "    net.train()  # enter train mode\n",
    "    loss_avg = 0.0\n",
    "\n",
    "    # start at a random point of the outlier dataset; this induces more randomness without obliterating locality\n",
    "    train_loader_out.dataset.offset = np.random.randint(len(train_loader_out.dataset))\n",
    "    for in_set, out_set in zip(train_loader_in, train_loader_out):\n",
    "        data = torch.cat((in_set[0], out_set[0]), 0)\n",
    "        target = in_set[1]\n",
    "\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # forward\n",
    "        x = net(data)\n",
    "\n",
    "        # backward\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = F.cross_entropy(x[:len(in_set[0])], target)\n",
    "        \n",
    "        ## OECC Loss Function\n",
    "        if args['dataset'] == 'cifar10':\n",
    "            A_tr = 0.94\n",
    "        elif args['dataset'] == 'cifar100':\n",
    "            A_tr = 0.78\n",
    "        sm = torch.nn.Softmax(dim=1) # Create a Softmax \n",
    "        probabilities = sm(x) # Get the probabilites for both In and Outlier Images\n",
    "        max_probs, _ = torch.max(probabilities, dim=1) # Take the maximum probabilities produced by softmax\n",
    "        prob_diff_in = max_probs[:len(in_set[0])] - A_tr  \n",
    "        loss += args['lambda_1'] * torch.sum(prob_diff_in**2) ## 1st Regularization term\n",
    "        prob_diff_out = probabilities[len(in_set[0]):][:] - (1/num_classes)\n",
    "        loss += args['lambda_2'] * torch.sum(torch.abs(prob_diff_out)) ## 2nd Regularization term\n",
    "                \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # exponential moving average\n",
    "        loss_avg = loss_avg * 0.8 + float(loss) * 0.2\n",
    "    \n",
    "    state['train_loss'] = loss_avg\n",
    "\n",
    "# test function\n",
    "def test():\n",
    "    net.eval()\n",
    "    loss_avg = 0.0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # forward\n",
    "            output = net(data)\n",
    "            loss = F.cross_entropy(output, target)\n",
    "\n",
    "            # accuracy\n",
    "            pred = output.data.max(1)[1]\n",
    "            correct += pred.eq(target.data).sum().item()\n",
    "\n",
    "            # test loss average\n",
    "            loss_avg += float(loss.data)            \n",
    "            \n",
    "    state['test_loss'] = loss_avg / len(test_loader)\n",
    "    state['test_accuracy'] = correct / len(test_loader.dataset)\n",
    "\n",
    "\n",
    "# Make save directory\n",
    "if not os.path.exists(args['save']):\n",
    "    os.makedirs(args['save'])\n",
    "if not os.path.isdir(args['save']):\n",
    "    raise Exception('%s is not a dir' % args['save'])\n",
    "\n",
    "with open(os.path.join(args['save'], args['dataset'] + calib_indicator + '_' + args['model'] +\n",
    "                                  '_Mahal_OECC_tune_training_results.csv'), 'w') as f:\n",
    "    f.write('epoch,time(s),train_loss,test_loss,test_error(%)\\n')\n",
    "\n",
    "print('Beginning Training\\n')\n",
    "\n",
    "\n",
    "# Main loop\n",
    "for epoch in range(0, args['epochs']):\n",
    "    state['epoch'] = epoch\n",
    "\n",
    "    begin_epoch = time.time()\n",
    "\n",
    "    train()\n",
    "    test()\n",
    "\n",
    "    # Save model\n",
    "    torch.save(net.state_dict(),\n",
    "               os.path.join(args['save'], args['dataset'] + calib_indicator + '_' + args['model'] +\n",
    "                            '_Mahal_OECC_tune_epoch_' + str(epoch) + '.pth'))\n",
    "    # Let us not waste space and delete the previous model\n",
    "    prev_path = os.path.join(args['save'], args['dataset'] + calib_indicator + '_' + args['model'] +\n",
    "                             '_Mahal_OECC_tune_epoch_' + str(epoch - 1) + '.pth')\n",
    "    if os.path.exists(prev_path): os.remove(prev_path)\n",
    "\n",
    "    # Show results\n",
    "\n",
    "    with open(os.path.join(args['save'], args['dataset'] + calib_indicator + '_' + args['model'] +\n",
    "                                      '_Mahal_OECC_tune_training_results.csv'), 'a') as f:\n",
    "        f.write('%03d,%05d,%0.6f,%0.5f,%0.2f\\n' % (\n",
    "            (epoch + 1),\n",
    "            time.time() - begin_epoch,\n",
    "            state['train_loss'],\n",
    "            state['test_loss'],\n",
    "            100 - 100. * state['test_accuracy'],\n",
    "        ))\n",
    "\n",
    "\n",
    "    print('Epoch {0:3d} | Time {1:5d} | Train Loss {2:.4f} | Test Loss {3:.3f} | Test Error {4:.2f}'.format(\n",
    "        (epoch + 1),\n",
    "        int(time.time() - begin_epoch),\n",
    "        state['train_loss'],\n",
    "        state['test_loss'],\n",
    "        100 - 100. * state['test_accuracy'])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "OUR_METHOD_oe_tune_cifar.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
